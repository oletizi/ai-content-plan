[["Map",1,2,7,8],"meta::meta",["Map",3,4,5,6],"astro-version","5.8.0","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://envoy-ai-gateway.io\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false},\"legacy\":{\"collections\":false}}","blog",["Map",9,10,17,18,34,35,48,49,62,63,76,77,90,91,104,105,118,119,132,133,146,147,160,161,188,189,202,203,216,217,230,231,240,241,254,255,268,269,318,319,332,333,346,347,360,361,398,399,412,413,426,427,436,437,444,445,458,459,472,473,486,487,500,501,514,515,524,525,558,559,568,569,578,579,588,589,626,627],"ai-gateway-vs-api-gateway",{"id":9,"data":11,"body":12,"filePath":13,"digest":14,"legacyId":15,"deferredRender":16},{},"title: \"AI Gateway vs API Gateway: What’s the Difference and Why It Matters\"\ndate: 2025-05-28\ndescription: \"Understand the key differences between traditional API gateways and emerging AI gateways designed for LLMOps.\"\n---\n\n# AI Gateway vs API Gateway: What’s the Difference and Why It Matters\n\nAPI gateways have become the default ingress layer for modern applications. But when it comes to AI workloads, they fall short:\n\n| Challenge             | API Gateway | AI Gateway |\n|----------------------|-------------|------------|\n| Token-based limits   | ❌          | ✅         |\n| Prompt observability | ❌          | ✅         |\n| AI-specific policies | ❌          | ✅         |\n| LLM cost control     | ❌          | ✅         |\n| Gateway API native   | ⚠️          | ✅         |\n\n## What Makes an AI Gateway Different?\n\nAI Gateways handle **model-specific traffic patterns**, including:\n\n- Large prompt payloads\n- High token counts\n- Tenant-specific policies\n- OpenAI/HuggingFace request formats\n\nThey’re **built for LLMOps**, not just REST and gRPC.\n\n## Why It Matters for Platform Teams\n\n- Prevent runaway usage of OpenAI, Claude, or TGI\n- Secure model access with OIDC, RBAC, and mTLS\n- Track usage down to the prompt or user level\n- Enable internal teams with self-serve, policy-governed access to models\n\n## Conclusion\n\nIf you're running LLMs in production, you need a gateway purpose-built for AI traffic.\n**Envoy AI Gateway** is that solution.\n\n🔗 [Learn more about Envoy AI Gateway](#)\n📘 [Read: “What is an AI Gateway?”](#)","src/content/blog/ai-gateway-vs-api-gateway.mdx","bcb1ee7e8d60fe63","ai-gateway-vs-api-gateway.mdx",true,"ai-api-cost-governance",{"id":17,"data":19,"body":23,"filePath":24,"digest":25,"rendered":26,"legacyId":33},{"title":20,"date":21,"description":22},"AI API Cost Governance: Lessons from Internal Platforms",["Date","2025-05-28T00:00:00.000Z"],"This post is coming soon!","*This article is part of our 30-day content series for Envoy AI Gateway. Stay tuned!*","src/content/blog/ai-api-cost-governance.md","0f1c686ce820e1a0",{"html":27,"metadata":28},"\u003Cp>\u003Cem>This article is part of our 30-day content series for Envoy AI Gateway. Stay tuned!\u003C/em>\u003C/p>",{"headings":29,"localImagePaths":30,"remoteImagePaths":31,"frontmatter":19,"imagePaths":32},[],[],[],[],"ai-api-cost-governance.md","ai-gateway-roadmap",{"id":34,"data":36,"body":23,"filePath":39,"digest":40,"rendered":41,"legacyId":47},{"title":37,"date":38,"description":22},"AI Gateway Roadmap: What’s Next for LLM Traffic Control",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/ai-gateway-roadmap.md","3fed68b62aa19f0c",{"html":27,"metadata":42},{"headings":43,"localImagePaths":44,"remoteImagePaths":45,"frontmatter":36,"imagePaths":46},[],[],[],[],"ai-gateway-roadmap.md","ai-platform-challenges",{"id":48,"data":50,"body":23,"filePath":53,"digest":54,"rendered":55,"legacyId":61},{"title":51,"date":52,"description":22},"5 Problems Every AI Platform Team Faces with LLMs",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/ai-platform-challenges.md","1e3acdfea703d54c",{"html":27,"metadata":56},{"headings":57,"localImagePaths":58,"remoteImagePaths":59,"frontmatter":50,"imagePaths":60},[],[],[],[],"ai-platform-challenges.md","compare-ingress-controllers",{"id":62,"data":64,"body":23,"filePath":67,"digest":68,"rendered":69,"legacyId":75},{"title":65,"date":66,"description":22},"Compare: Envoy Gateway vs Kong vs NGINX for AI Ingress",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/compare-ingress-controllers.md","8d4dd474176dac71",{"html":27,"metadata":70},{"headings":71,"localImagePaths":72,"remoteImagePaths":73,"frontmatter":64,"imagePaths":74},[],[],[],[],"compare-ingress-controllers.md","fintech-case-study",{"id":76,"data":78,"body":23,"filePath":81,"digest":82,"rendered":83,"legacyId":89},{"title":79,"date":80,"description":22},"How a Fintech Platform Team Secured LLMs with Envoy AI Gateway",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/fintech-case-study.md","2e81b32e6a9f827e",{"html":27,"metadata":84},{"headings":85,"localImagePaths":86,"remoteImagePaths":87,"frontmatter":78,"imagePaths":88},[],[],[],[],"fintech-case-study.md","gateway-api-for-ai",{"id":90,"data":92,"body":23,"filePath":95,"digest":96,"rendered":97,"legacyId":103},{"title":93,"date":94,"description":22},"A Guide to the Kubernetes Gateway API for AI Traffic",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/gateway-api-for-ai.md","e7bdf51edf828f51",{"html":27,"metadata":98},{"headings":99,"localImagePaths":100,"remoteImagePaths":101,"frontmatter":92,"imagePaths":102},[],[],[],[],"gateway-api-for-ai.md","gitops-argo",{"id":104,"data":106,"body":23,"filePath":109,"digest":110,"rendered":111,"legacyId":117},{"title":107,"date":108,"description":22},"Deploying Envoy AI Gateway with GitOps and Argo CD",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/gitops-argo.md","91c27400f973efa7",{"html":27,"metadata":112},{"headings":113,"localImagePaths":114,"remoteImagePaths":115,"frontmatter":106,"imagePaths":116},[],[],[],[],"gitops-argo.md","cut-llm-costs",{"id":118,"data":120,"body":23,"filePath":123,"digest":124,"rendered":125,"legacyId":131},{"title":121,"date":122,"description":22},"How We Cut Our LLM Bill by 40% With Envoy Gateway",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/cut-llm-costs.md","8354fff407f1e7bd",{"html":27,"metadata":126},{"headings":127,"localImagePaths":128,"remoteImagePaths":129,"frontmatter":120,"imagePaths":130},[],[],[],[],"cut-llm-costs.md","gke-deploy-guide",{"id":132,"data":134,"body":23,"filePath":137,"digest":138,"rendered":139,"legacyId":145},{"title":135,"date":136,"description":22},"Deploy Envoy AI Gateway in GKE with Gateway API",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/gke-deploy-guide.md","93529ae19aebcb07",{"html":27,"metadata":140},{"headings":141,"localImagePaths":142,"remoteImagePaths":143,"frontmatter":134,"imagePaths":144},[],[],[],[],"gke-deploy-guide.md","huggingface-secure",{"id":146,"data":148,"body":23,"filePath":151,"digest":152,"rendered":153,"legacyId":159},{"title":149,"date":150,"description":22},"Secure Hugging Face TGI with Auth and Rate Limits",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/huggingface-secure.md","6079819c70c8b965",{"html":27,"metadata":154},{"headings":155,"localImagePaths":156,"remoteImagePaths":157,"frontmatter":148,"imagePaths":158},[],[],[],[],"huggingface-secure.md","introducing-envoy-ai-gateway",{"id":160,"data":162,"body":166,"filePath":167,"digest":168,"rendered":169,"legacyId":187},{"title":163,"date":164,"description":165},"Introducing Envoy AI Gateway: Secure, Scalable AI Inference for Kubernetes",["Date","2025-05-28T00:00:00.000Z"],"Envoy AI Gateway provides a secure, scalable, Kubernetes-native way to manage access to AI model endpoints.","AI models are powering everything from chatbots to copilots—but most teams are exposing them through insecure, unobservable, and expensive APIs. Envoy AI Gateway solves this by putting AI access behind a secure, policy-aware, and Kubernetes-native gateway.\n\n## The Problem\n\nProductionizing AI brings serious challenges:\n\n- No easy way to apply authentication or rate limits\n- Token sprawl and runaway inference costs\n- Lack of observability into model usage—or abuse\n\n## The Solution\n\n**Envoy AI Gateway** is built on Envoy Proxy and the Kubernetes Gateway API. It gives platform teams a modern, secure, and extensible way to manage AI inference traffic.\n\n### Key Benefits\n\n- ✅ Enforce mTLS, OIDC, and RBAC for LLM endpoints\n- ✅ Control usage with token-aware rate limits and quotas\n- ✅ Observe prompt traffic, latency, and errors\n- ✅ Native Kubernetes and GitOps support","src/content/blog/introducing-envoy-ai-gateway.md","10aecf8b22cd5d65",{"html":170,"metadata":171},"\u003Cp>AI models are powering everything from chatbots to copilots—but most teams are exposing them through insecure, unobservable, and expensive APIs. Envoy AI Gateway solves this by putting AI access behind a secure, policy-aware, and Kubernetes-native gateway.\u003C/p>\n\u003Ch2 id=\"the-problem\">The Problem\u003C/h2>\n\u003Cp>Productionizing AI brings serious challenges:\u003C/p>\n\u003Cul>\n\u003Cli>No easy way to apply authentication or rate limits\u003C/li>\n\u003Cli>Token sprawl and runaway inference costs\u003C/li>\n\u003Cli>Lack of observability into model usage—or abuse\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"the-solution\">The Solution\u003C/h2>\n\u003Cp>\u003Cstrong>Envoy AI Gateway\u003C/strong> is built on Envoy Proxy and the Kubernetes Gateway API. It gives platform teams a modern, secure, and extensible way to manage AI inference traffic.\u003C/p>\n\u003Ch3 id=\"key-benefits\">Key Benefits\u003C/h3>\n\u003Cul>\n\u003Cli>✅ Enforce mTLS, OIDC, and RBAC for LLM endpoints\u003C/li>\n\u003Cli>✅ Control usage with token-aware rate limits and quotas\u003C/li>\n\u003Cli>✅ Observe prompt traffic, latency, and errors\u003C/li>\n\u003Cli>✅ Native Kubernetes and GitOps support\u003C/li>\n\u003C/ul>",{"headings":172,"localImagePaths":184,"remoteImagePaths":185,"frontmatter":162,"imagePaths":186},[173,177,180],{"depth":174,"slug":175,"text":176},2,"the-problem","The Problem",{"depth":174,"slug":178,"text":179},"the-solution","The Solution",{"depth":181,"slug":182,"text":183},3,"key-benefits","Key Benefits",[],[],[],"introducing-envoy-ai-gateway.md","langchain-token-budgets",{"id":188,"data":190,"body":23,"filePath":193,"digest":194,"rendered":195,"legacyId":201},{"title":191,"date":192,"description":22},"Building Token Budgets for LangChain Agents",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/langchain-token-budgets.md","d8402b0a67bb964a",{"html":27,"metadata":196},{"headings":197,"localImagePaths":198,"remoteImagePaths":199,"frontmatter":190,"imagePaths":200},[],[],[],[],"langchain-token-budgets.md","langserve-fastapi",{"id":202,"data":204,"body":23,"filePath":207,"digest":208,"rendered":209,"legacyId":215},{"title":205,"date":206,"description":22},"Using Envoy Gateway with LangServe and FastAPI",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/langserve-fastapi.md","cd1fc71cf4d2f0f2",{"html":27,"metadata":210},{"headings":211,"localImagePaths":212,"remoteImagePaths":213,"frontmatter":204,"imagePaths":214},[],[],[],[],"langserve-fastapi.md","multi-tenant-inference",{"id":216,"data":218,"body":23,"filePath":221,"digest":222,"rendered":223,"legacyId":229},{"title":219,"date":220,"description":22},"Envoy Gateway for Multi-Tenant AI Inference",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/multi-tenant-inference.md","7988086e9c44d8ee",{"html":27,"metadata":224},{"headings":225,"localImagePaths":226,"remoteImagePaths":227,"frontmatter":218,"imagePaths":228},[],[],[],[],"multi-tenant-inference.md","how-to-rate-limit-openai-access-in-kubernetes-using-envoy-ai-gateway",{"id":230,"data":232,"body":236,"filePath":237,"digest":238,"legacyId":239,"deferredRender":16},{"title":233,"date":234,"description":235},"How to Rate Limit OpenAI Access in Kubernetes Using Envoy AI Gateway",["Date","2025-05-28T00:00:00.000Z"],"Step-by-step tutorial to apply rate limits to OpenAI's API using Envoy AI Gateway in Kubernetes.","# How to Rate Limit OpenAI Access in Kubernetes Using Envoy AI Gateway\n\nLLMs are powerful—and expensive. Without controls, a single integration can burn through your OpenAI quota in minutes.\n\n**Envoy AI Gateway** helps you:\n\n- Intercept and control requests to OpenAI\n- Apply per-user or per-service quotas\n- Track request and token usage\n\n## What You’ll Learn\n- How to deploy Envoy AI Gateway in your cluster\n- Define Gateway API resources with rate-limiting policies\n- Configure token budgets and enforce per-tenant usage\n- Monitor usage with built-in telemetry\n\n## Example Policy\n```yaml\nrateLimits:\n  - name: openai-limit\n    match:\n      path: /v1/chat/completions\n    requestsPerUnit: 100\n    unit: minute\n```\n\nThis allows you to safely expose OpenAI while keeping costs under control.\n\n**[Follow the full tutorial](#)** | **[Join the community](#)** | **[Deploy secure AI inference](#)**","src/content/blog/how-to-rate-limit-openai-access-in-kubernetes-using-envoy-ai-gateway.mdx","0db25eb8e9166e03","how-to-rate-limit-openai-access-in-kubernetes-using-envoy-ai-gateway.mdx","observe-prompt-latency",{"id":240,"data":242,"body":23,"filePath":245,"digest":246,"rendered":247,"legacyId":253},{"title":243,"date":244,"description":22},"How to Observe Prompt Latency and Errors with Envoy",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/observe-prompt-latency.md","21914831adf6a520",{"html":27,"metadata":248},{"headings":249,"localImagePaths":250,"remoteImagePaths":251,"frontmatter":242,"imagePaths":252},[],[],[],[],"observe-prompt-latency.md","prompt-firewall",{"id":254,"data":256,"body":23,"filePath":259,"digest":260,"rendered":261,"legacyId":267},{"title":257,"date":258,"description":22},"Build a Prompt Firewall with Envoy Gateway",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/prompt-firewall.md","e174dade26d84e76",{"html":27,"metadata":262},{"headings":263,"localImagePaths":264,"remoteImagePaths":265,"frontmatter":256,"imagePaths":266},[],[],[],[],"prompt-firewall.md","rate-limit-openai-prompts",{"id":268,"data":270,"body":274,"filePath":275,"digest":276,"rendered":277,"legacyId":317},{"title":271,"date":272,"description":273},"Rate-Limit OpenAI Prompt Usage per Team with Envoy AI Gateway",["Date","2025-05-28T00:00:00.000Z"],"Learn how to apply per-team rate limits to OpenAI API traffic using Envoy AI Gateway in a Kubernetes environment.","# Rate-Limit OpenAI Prompt Usage per Team with Envoy AI Gateway\n\nPrompt usage across multiple teams can quickly spiral out of control—especially when powered by high-cost APIs like OpenAI’s. Without boundaries, a single app or developer can:\n\n- Exceed organizational token budgets\n- Starve other teams of model access\n- Generate surprise costs at the end of the month\n\n**Envoy AI Gateway** allows you to place fine-grained, policy-driven controls around OpenAI access—including request limits per team, service, or user.\n\n## Why Per-Team Rate Limiting?\n\nRate limiting by IP or global quota isn’t enough in multi-team environments. Instead, you want to:\n\n- Allocate different limits per team (e.g., marketing, support, R&D)\n- Prevent noisy neighbors from disrupting others\n- Provide predictability for usage and billing\n\n## Architecture: Where Rate Limits Live\n\n```\n[ Client / App ] --> [ Envoy AI Gateway ] --> [ OpenAI API ]\n                          |\n                     Per-team policies\n               (based on headers or JWT claims)\n```\n\n## Step 1: Identify Teams\n\nYou can identify the calling team in one of two ways:\n\n### Option 1: JWT Claims\nIf using OIDC, team identity may come from a `group` or `tenant_id` claim.\n\n### Option 2: Request Header\nA custom header like `X-Team-ID` works for internal clients.\n\n## Step 2: Define Rate Limit Policies\n\nHere’s an example using request headers:\n\n```yaml\nrateLimits:\n  - name: marketing-team\n    match:\n      headers:\n        X-Team-ID: marketing\n      path: /v1/chat/completions\n    requestsPerUnit: 1000\n    unit: day\n\n  - name: research-team\n    match:\n      headers:\n        X-Team-ID: research\n      path: /v1/chat/completions\n    requestsPerUnit: 10000\n    unit: day\n```\n\n## Step 3: Apply Defaults and Fail Safes\n\nTo prevent abuse or misconfigurations:\n\n- Set a default rate limit for unidentified traffic\n- Use fallback policies to throttle excess usage\n- Log limit hits and overages for audit and tuning\n\n## Bonus: Token-Based Quotas\n\nIn addition to request count, you can define limits based on total tokens consumed:\n\n```yaml\ntokenBudgets:\n  - tenant: marketing\n    maxTokens: 50000\n    period: month\n```\n\nThis is particularly useful when OpenAI usage varies by prompt size and model.\n\n## Observability: Know Who’s Hitting the Limits\n\nEnvoy AI Gateway integrates with:\n\n- **Prometheus** for metrics\n- **Grafana** dashboards by team\n- **OpenTelemetry** for tracing model requests\n\nMonitor:\n- Which teams are using the most prompts\n- Where limits are being hit\n- Latency and error rates per team\n\n## Conclusion\n\nRate limiting isn’t just about infrastructure protection anymore—it’s a vital control layer for LLM cost management, fairness, and predictability.\n\nEnvoy AI Gateway makes it easy to:\n- Apply per-team usage policies\n- Limit OpenAI prompt access by request or token\n- Observe and adjust limits with confidence\n\n---\n\n⚖️ **Enforce fairness and control in your AI platform**  \n📘 [Read the rate-limiting configuration guide](#)  \n🚀 [Try Envoy AI Gateway today](#)","src/content/blog/rate-limit-openai-prompts.md","89c6a77b6598f149",{"html":278,"metadata":279},"\u003Ch1 id=\"rate-limit-openai-prompt-usage-per-team-with-envoy-ai-gateway\">Rate-Limit OpenAI Prompt Usage per Team with Envoy AI Gateway\u003C/h1>\n\u003Cp>Prompt usage across multiple teams can quickly spiral out of control—especially when powered by high-cost APIs like OpenAI’s. Without boundaries, a single app or developer can:\u003C/p>\n\u003Cul>\n\u003Cli>Exceed organizational token budgets\u003C/li>\n\u003Cli>Starve other teams of model access\u003C/li>\n\u003Cli>Generate surprise costs at the end of the month\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Envoy AI Gateway\u003C/strong> allows you to place fine-grained, policy-driven controls around OpenAI access—including request limits per team, service, or user.\u003C/p>\n\u003Ch2 id=\"why-per-team-rate-limiting\">Why Per-Team Rate Limiting?\u003C/h2>\n\u003Cp>Rate limiting by IP or global quota isn’t enough in multi-team environments. Instead, you want to:\u003C/p>\n\u003Cul>\n\u003Cli>Allocate different limits per team (e.g., marketing, support, R&#x26;D)\u003C/li>\n\u003Cli>Prevent noisy neighbors from disrupting others\u003C/li>\n\u003Cli>Provide predictability for usage and billing\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"architecture-where-rate-limits-live\">Architecture: Where Rate Limits Live\u003C/h2>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>[ Client / App ] --> [ Envoy AI Gateway ] --> [ OpenAI API ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>                          |\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>                     Per-team policies\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>               (based on headers or JWT claims)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"step-1-identify-teams\">Step 1: Identify Teams\u003C/h2>\n\u003Cp>You can identify the calling team in one of two ways:\u003C/p>\n\u003Ch3 id=\"option-1-jwt-claims\">Option 1: JWT Claims\u003C/h3>\n\u003Cp>If using OIDC, team identity may come from a \u003Ccode>group\u003C/code> or \u003Ccode>tenant_id\u003C/code> claim.\u003C/p>\n\u003Ch3 id=\"option-2-request-header\">Option 2: Request Header\u003C/h3>\n\u003Cp>A custom header like \u003Ccode>X-Team-ID\u003C/code> works for internal clients.\u003C/p>\n\u003Ch2 id=\"step-2-define-rate-limit-policies\">Step 2: Define Rate Limit Policies\u003C/h2>\n\u003Cp>Here’s an example using request headers:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"yaml\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">rateLimits\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  - \u003C/span>\u003Cspan style=\"color:#85E89D\">name\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">marketing-team\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    match\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      headers\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">        X-Team-ID\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">marketing\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      path\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">/v1/chat/completions\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    requestsPerUnit\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">1000\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    unit\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">day\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  - \u003C/span>\u003Cspan style=\"color:#85E89D\">name\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">research-team\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    match\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      headers\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">        X-Team-ID\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">research\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      path\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">/v1/chat/completions\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    requestsPerUnit\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">10000\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    unit\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">day\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"step-3-apply-defaults-and-fail-safes\">Step 3: Apply Defaults and Fail Safes\u003C/h2>\n\u003Cp>To prevent abuse or misconfigurations:\u003C/p>\n\u003Cul>\n\u003Cli>Set a default rate limit for unidentified traffic\u003C/li>\n\u003Cli>Use fallback policies to throttle excess usage\u003C/li>\n\u003Cli>Log limit hits and overages for audit and tuning\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"bonus-token-based-quotas\">Bonus: Token-Based Quotas\u003C/h2>\n\u003Cp>In addition to request count, you can define limits based on total tokens consumed:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"yaml\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">tokenBudgets\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  - \u003C/span>\u003Cspan style=\"color:#85E89D\">tenant\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">marketing\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    maxTokens\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">50000\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    period\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">month\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This is particularly useful when OpenAI usage varies by prompt size and model.\u003C/p>\n\u003Ch2 id=\"observability-know-whos-hitting-the-limits\">Observability: Know Who’s Hitting the Limits\u003C/h2>\n\u003Cp>Envoy AI Gateway integrates with:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Prometheus\u003C/strong> for metrics\u003C/li>\n\u003Cli>\u003Cstrong>Grafana\u003C/strong> dashboards by team\u003C/li>\n\u003Cli>\u003Cstrong>OpenTelemetry\u003C/strong> for tracing model requests\u003C/li>\n\u003C/ul>\n\u003Cp>Monitor:\u003C/p>\n\u003Cul>\n\u003Cli>Which teams are using the most prompts\u003C/li>\n\u003Cli>Where limits are being hit\u003C/li>\n\u003Cli>Latency and error rates per team\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>Rate limiting isn’t just about infrastructure protection anymore—it’s a vital control layer for LLM cost management, fairness, and predictability.\u003C/p>\n\u003Cp>Envoy AI Gateway makes it easy to:\u003C/p>\n\u003Cul>\n\u003Cli>Apply per-team usage policies\u003C/li>\n\u003Cli>Limit OpenAI prompt access by request or token\u003C/li>\n\u003Cli>Observe and adjust limits with confidence\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Cp>⚖️ \u003Cstrong>Enforce fairness and control in your AI platform\u003C/strong>\u003Cbr>\n📘 \u003Ca href=\"#\">Read the rate-limiting configuration guide\u003C/a>\u003Cbr>\n🚀 \u003Ca href=\"#\">Try Envoy AI Gateway today\u003C/a>\u003C/p>",{"headings":280,"localImagePaths":314,"remoteImagePaths":315,"frontmatter":270,"imagePaths":316},[281,284,287,290,293,296,299,302,305,308,311],{"depth":282,"slug":283,"text":271},1,"rate-limit-openai-prompt-usage-per-team-with-envoy-ai-gateway",{"depth":174,"slug":285,"text":286},"why-per-team-rate-limiting","Why Per-Team Rate Limiting?",{"depth":174,"slug":288,"text":289},"architecture-where-rate-limits-live","Architecture: Where Rate Limits Live",{"depth":174,"slug":291,"text":292},"step-1-identify-teams","Step 1: Identify Teams",{"depth":181,"slug":294,"text":295},"option-1-jwt-claims","Option 1: JWT Claims",{"depth":181,"slug":297,"text":298},"option-2-request-header","Option 2: Request Header",{"depth":174,"slug":300,"text":301},"step-2-define-rate-limit-policies","Step 2: Define Rate Limit Policies",{"depth":174,"slug":303,"text":304},"step-3-apply-defaults-and-fail-safes","Step 3: Apply Defaults and Fail Safes",{"depth":174,"slug":306,"text":307},"bonus-token-based-quotas","Bonus: Token-Based Quotas",{"depth":174,"slug":309,"text":310},"observability-know-whos-hitting-the-limits","Observability: Know Who’s Hitting the Limits",{"depth":174,"slug":312,"text":313},"conclusion","Conclusion",[],[],[],"rate-limit-openai-prompts.md","mtls-internal-llms",{"id":318,"data":320,"body":23,"filePath":323,"digest":324,"rendered":325,"legacyId":331},{"title":321,"date":322,"description":22},"mTLS for Internal LLM APIs with Envoy Gateway",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/mtls-internal-llms.md","de5c1483dd91eb9a",{"html":27,"metadata":326},{"headings":327,"localImagePaths":328,"remoteImagePaths":329,"frontmatter":320,"imagePaths":330},[],[],[],[],"mtls-internal-llms.md","rise-of-llmops",{"id":332,"data":334,"body":23,"filePath":337,"digest":338,"rendered":339,"legacyId":345},{"title":335,"date":336,"description":22},"The Rise of LLMOps and Why It Needs a Gateway",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/rise-of-llmops.md","0ad01db78f4f619a",{"html":27,"metadata":340},{"headings":341,"localImagePaths":342,"remoteImagePaths":343,"frontmatter":334,"imagePaths":344},[],[],[],[],"rise-of-llmops.md","secure-ai-regulated",{"id":346,"data":348,"body":23,"filePath":351,"digest":352,"rendered":353,"legacyId":359},{"title":349,"date":350,"description":22},"Best Practices for Securing AI APIs in Regulated Industries",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/secure-ai-regulated.md","28c2cfe343e723eb",{"html":27,"metadata":354},{"headings":355,"localImagePaths":356,"remoteImagePaths":357,"frontmatter":348,"imagePaths":358},[],[],[],[],"secure-ai-regulated.md","secure-openai-oidc",{"id":360,"data":362,"body":366,"filePath":367,"digest":368,"rendered":369,"legacyId":397},{"title":363,"date":364,"description":365},"Secure OpenAI Access with OIDC in Kubernetes Using Envoy AI Gateway",["Date","2025-05-28T00:00:00.000Z"],"Learn how to authenticate and control access to OpenAI’s API using OIDC and Envoy AI Gateway in a Kubernetes environment.","# Secure OpenAI Access with OIDC in Kubernetes Using Envoy AI Gateway\n\nAs generative AI becomes integral to internal tools and user-facing products, securing access to model APIs like OpenAI’s becomes critical. Without proper controls, teams risk:\n\n- Unauthenticated access to high-cost APIs\n- Loss of visibility into who is prompting models\n- Data exposure from unverified users\n- Difficulty enforcing usage limits or quotas\n\nIn this guide, you’ll learn how to use **Envoy AI Gateway** with **OIDC (OpenID Connect)** to secure access to OpenAI's API from within your Kubernetes environment.\n\n## Why OIDC?\n\nOIDC is a widely adopted identity layer built on OAuth 2.0. It allows users to authenticate using identity providers like:\n\n- Google Workspace\n- Okta\n- Auth0\n- Azure AD\n\nWith OIDC in place, you can:\n- Enforce login for any prompt or API request\n- Identify users via JWT claims\n- Apply policy per identity or group\n- Audit access logs for security and billing\n\n## Architecture Overview\n\n```\n[ Dev Portal / CLI ]\n        |\n    [ OIDC Login ]\n        |\n[ Envoy AI Gateway ] --> [ OpenAI API ]\n        |\n     Policies\n   (rate limits, auth)\n```\n\n## Step 1: Configure Envoy AI Gateway with OIDC\n\nExample Gateway configuration using OIDC:\n\n```yaml\nauthentication:\n  - provider:\n      name: oidc-auth\n      issuer: https://auth.myorg.com\n      clientId: envoy-gateway\n      jwksUri: https://auth.myorg.com/.well-known/jwks.json\n```\n\nYou can enforce this policy on any route (e.g., `/v1/chat/completions`) and require a valid bearer token signed by your IdP.\n\n## Step 2: Map JWT Claims to Policy\n\nWith OIDC tokens available at the gateway, you can apply advanced controls:\n\n- Rate limits by `email` or `sub` (subject ID)\n- RBAC by `groups` claim\n- Token quotas by `tenant_id` or `project_id`\n\nExample policy snippet:\n\n```yaml\nrateLimits:\n  - name: user-limit\n    match:\n      path: /v1/chat/completions\n      claims:\n        email: \"*@myorg.com\"\n    requestsPerUnit: 100\n    unit: minute\n```\n\n## Step 3: Enforce Logging and Auditing\n\nEnvoy AI Gateway integrates with Prometheus and OpenTelemetry to expose:\n\n- Which user made each request\n- Which endpoints were hit\n- Request latency and failure rates\n- JWT claims used to authorize traffic\n\n## Bonus: Multi-Tenant Support\n\nOIDC + Envoy AI Gateway makes it easy to isolate access by team, project, or product by parsing tenant IDs from JWT claims or request headers.\n\nExample: apply a token budget per team:\n\n```yaml\ntokenBudgets:\n  - tenant: marketing\n    maxTokens: 100000\n    period: month\n```\n\n## Conclusion\n\nWith just a few lines of configuration, you can enforce secure, authenticated, and observable access to OpenAI—without writing custom middleware or proxies.\n\nEnvoy AI Gateway makes it easy to:\n- Enforce enterprise SSO and zero-trust policies\n- Set per-user or per-group usage controls\n- Observe LLM usage patterns at the edge\n\n---\n\n🔐 **Secure your OpenAI usage today**  \n📘 [See the full OIDC configuration guide](#)  \n🚀 [Deploy Envoy AI Gateway in your cluster](#)","src/content/blog/secure-openai-oidc.md","7d083d9f9b4bfe22",{"html":370,"metadata":371},"\u003Ch1 id=\"secure-openai-access-with-oidc-in-kubernetes-using-envoy-ai-gateway\">Secure OpenAI Access with OIDC in Kubernetes Using Envoy AI Gateway\u003C/h1>\n\u003Cp>As generative AI becomes integral to internal tools and user-facing products, securing access to model APIs like OpenAI’s becomes critical. Without proper controls, teams risk:\u003C/p>\n\u003Cul>\n\u003Cli>Unauthenticated access to high-cost APIs\u003C/li>\n\u003Cli>Loss of visibility into who is prompting models\u003C/li>\n\u003Cli>Data exposure from unverified users\u003C/li>\n\u003Cli>Difficulty enforcing usage limits or quotas\u003C/li>\n\u003C/ul>\n\u003Cp>In this guide, you’ll learn how to use \u003Cstrong>Envoy AI Gateway\u003C/strong> with \u003Cstrong>OIDC (OpenID Connect)\u003C/strong> to secure access to OpenAI’s API from within your Kubernetes environment.\u003C/p>\n\u003Ch2 id=\"why-oidc\">Why OIDC?\u003C/h2>\n\u003Cp>OIDC is a widely adopted identity layer built on OAuth 2.0. It allows users to authenticate using identity providers like:\u003C/p>\n\u003Cul>\n\u003Cli>Google Workspace\u003C/li>\n\u003Cli>Okta\u003C/li>\n\u003Cli>Auth0\u003C/li>\n\u003Cli>Azure AD\u003C/li>\n\u003C/ul>\n\u003Cp>With OIDC in place, you can:\u003C/p>\n\u003Cul>\n\u003Cli>Enforce login for any prompt or API request\u003C/li>\n\u003Cli>Identify users via JWT claims\u003C/li>\n\u003Cli>Apply policy per identity or group\u003C/li>\n\u003Cli>Audit access logs for security and billing\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"architecture-overview\">Architecture Overview\u003C/h2>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>[ Dev Portal / CLI ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>        |\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>    [ OIDC Login ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>        |\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>[ Envoy AI Gateway ] --> [ OpenAI API ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>        |\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>     Policies\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>   (rate limits, auth)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"step-1-configure-envoy-ai-gateway-with-oidc\">Step 1: Configure Envoy AI Gateway with OIDC\u003C/h2>\n\u003Cp>Example Gateway configuration using OIDC:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"yaml\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">authentication\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  - \u003C/span>\u003Cspan style=\"color:#85E89D\">provider\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      name\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">oidc-auth\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      issuer\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">https://auth.myorg.com\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      clientId\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">envoy-gateway\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      jwksUri\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">https://auth.myorg.com/.well-known/jwks.json\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>You can enforce this policy on any route (e.g., \u003Ccode>/v1/chat/completions\u003C/code>) and require a valid bearer token signed by your IdP.\u003C/p>\n\u003Ch2 id=\"step-2-map-jwt-claims-to-policy\">Step 2: Map JWT Claims to Policy\u003C/h2>\n\u003Cp>With OIDC tokens available at the gateway, you can apply advanced controls:\u003C/p>\n\u003Cul>\n\u003Cli>Rate limits by \u003Ccode>email\u003C/code> or \u003Ccode>sub\u003C/code> (subject ID)\u003C/li>\n\u003Cli>RBAC by \u003Ccode>groups\u003C/code> claim\u003C/li>\n\u003Cli>Token quotas by \u003Ccode>tenant_id\u003C/code> or \u003Ccode>project_id\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Cp>Example policy snippet:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"yaml\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">rateLimits\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  - \u003C/span>\u003Cspan style=\"color:#85E89D\">name\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">user-limit\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    match\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      path\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">/v1/chat/completions\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      claims\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">        email\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"*@myorg.com\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    requestsPerUnit\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">100\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    unit\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">minute\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"step-3-enforce-logging-and-auditing\">Step 3: Enforce Logging and Auditing\u003C/h2>\n\u003Cp>Envoy AI Gateway integrates with Prometheus and OpenTelemetry to expose:\u003C/p>\n\u003Cul>\n\u003Cli>Which user made each request\u003C/li>\n\u003Cli>Which endpoints were hit\u003C/li>\n\u003Cli>Request latency and failure rates\u003C/li>\n\u003Cli>JWT claims used to authorize traffic\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"bonus-multi-tenant-support\">Bonus: Multi-Tenant Support\u003C/h2>\n\u003Cp>OIDC + Envoy AI Gateway makes it easy to isolate access by team, project, or product by parsing tenant IDs from JWT claims or request headers.\u003C/p>\n\u003Cp>Example: apply a token budget per team:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"yaml\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">tokenBudgets\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  - \u003C/span>\u003Cspan style=\"color:#85E89D\">tenant\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">marketing\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    maxTokens\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">100000\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    period\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">month\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>With just a few lines of configuration, you can enforce secure, authenticated, and observable access to OpenAI—without writing custom middleware or proxies.\u003C/p>\n\u003Cp>Envoy AI Gateway makes it easy to:\u003C/p>\n\u003Cul>\n\u003Cli>Enforce enterprise SSO and zero-trust policies\u003C/li>\n\u003Cli>Set per-user or per-group usage controls\u003C/li>\n\u003Cli>Observe LLM usage patterns at the edge\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Cp>🔐 \u003Cstrong>Secure your OpenAI usage today\u003C/strong>\u003Cbr>\n📘 \u003Ca href=\"#\">See the full OIDC configuration guide\u003C/a>\u003Cbr>\n🚀 \u003Ca href=\"#\">Deploy Envoy AI Gateway in your cluster\u003C/a>\u003C/p>",{"headings":372,"localImagePaths":394,"remoteImagePaths":395,"frontmatter":362,"imagePaths":396},[373,375,378,381,384,387,390,393],{"depth":282,"slug":374,"text":363},"secure-openai-access-with-oidc-in-kubernetes-using-envoy-ai-gateway",{"depth":174,"slug":376,"text":377},"why-oidc","Why OIDC?",{"depth":174,"slug":379,"text":380},"architecture-overview","Architecture Overview",{"depth":174,"slug":382,"text":383},"step-1-configure-envoy-ai-gateway-with-oidc","Step 1: Configure Envoy AI Gateway with OIDC",{"depth":174,"slug":385,"text":386},"step-2-map-jwt-claims-to-policy","Step 2: Map JWT Claims to Policy",{"depth":174,"slug":388,"text":389},"step-3-enforce-logging-and-auditing","Step 3: Enforce Logging and Auditing",{"depth":174,"slug":391,"text":392},"bonus-multi-tenant-support","Bonus: Multi-Tenant Support",{"depth":174,"slug":312,"text":313},[],[],[],"secure-openai-oidc.md","self-service-ai",{"id":398,"data":400,"body":23,"filePath":403,"digest":404,"rendered":405,"legacyId":411},{"title":401,"date":402,"description":22},"Enabling Self-Service AI Access for Internal Devs",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/self-service-ai.md","8d3c3a7617b625ae",{"html":27,"metadata":406},{"headings":407,"localImagePaths":408,"remoteImagePaths":409,"frontmatter":400,"imagePaths":410},[],[],[],[],"self-service-ai.md","token-accounting-custom-llms",{"id":412,"data":414,"body":23,"filePath":417,"digest":418,"rendered":419,"legacyId":425},{"title":415,"date":416,"description":22},"Token Accounting for Custom LLM APIs (e.g. Mistral)",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/token-accounting-custom-llms.md","18d80e52c482aecd",{"html":27,"metadata":420},{"headings":421,"localImagePaths":422,"remoteImagePaths":423,"frontmatter":414,"imagePaths":424},[],[],[],[],"token-accounting-custom-llms.md","secure-hugging-face-tgi-with-envoy-ai-gateway",{"id":426,"data":428,"body":432,"filePath":433,"digest":434,"legacyId":435,"deferredRender":16},{"title":429,"date":430,"description":431},"Secure Hugging Face TGI with Envoy AI Gateway: Auth, Rate Limits, and Observability",["Date","2025-05-28T00:00:00.000Z"],"Deploy Hugging Face’s TGI behind Envoy AI Gateway to control access, enforce rate limits, and monitor usage in Kubernetes.","# Secure Hugging Face TGI with Envoy AI Gateway: Auth, Rate Limits, and Observability\n\nDeploying a hosted LLM like Hugging Face TGI is only the beginning. To make it usable in production, you need:\n\n- Authentication to restrict who can prompt the model\n- Rate limits to avoid accidental (or intentional) overload\n- Observability to know how it's being used\n\n**Envoy AI Gateway** makes all of that easy—and Kubernetes-native.\n\n## In This Tutorial, You'll Learn\n\n- How to deploy Hugging Face TGI in Kubernetes\n- How to route traffic through Envoy AI Gateway\n- How to apply mTLS or OIDC authentication\n- How to add per-user rate limiting\n- How to collect usage metrics for prompts and responses\n\n### Architecture Diagram\n```\n[ App ] --> [ Envoy AI Gateway ] --> [ Hugging Face TGI ]\n                 |                      |\n           Policies + Auth          Model Output\n```\n\n## Sample Gateway API Policy\n```yaml\nrateLimits:\n  - name: tgi-user-limit\n    match:\n      path: /generate\n    requestsPerUnit: 50\n    unit: minute\n```\n\n## Bonus Features\n\n- ✅ Use Prometheus and Grafana to chart token usage\n- ✅ Extend with Wasm filters to redact prompts or mask PII\n\n**[Deploy the Example](#)** | **[Join the Community](#)**","src/content/blog/secure-hugging-face-tgi-with-envoy-ai-gateway.mdx","1c9eb9e098ad6aed","secure-hugging-face-tgi-with-envoy-ai-gateway.mdx","secure-scalable-ai-inference-for-kubernetes",{"id":436,"data":438,"body":440,"filePath":441,"digest":442,"legacyId":443,"deferredRender":16},{"title":163,"date":439,"description":165},["Date","2025-05-28T00:00:00.000Z"],"# Introducing Envoy AI Gateway: Secure, Scalable AI Inference for Kubernetes\n\nAI models are powering everything from chatbots to copilots—but most teams are exposing them through insecure, unobservable, and expensive APIs. Envoy AI Gateway solves this by putting AI access behind a secure, policy-aware, and Kubernetes-native gateway.\n\n## The Problem\nProductionizing AI brings serious challenges:\n\n- No easy way to apply authentication or rate limits\n- Token sprawl and runaway inference costs\n- Lack of observability into model usage—or abuse\n\n## The Solution\n**Envoy AI Gateway** is built on Envoy Proxy and the Kubernetes Gateway API. It gives platform teams a modern, secure, and extensible way to manage AI inference traffic.\n\n### Key Benefits\n- ✅ Enforce mTLS, OIDC, and RBAC for LLM endpoints\n- ✅ Control usage with token-aware rate limits and quotas\n- ✅ Observe prompt traffic, latency, and errors\n- ✅ Native Kubernetes and GitOps support\n\nWhether you're managing OpenAI usage or serving your own models, Envoy AI Gateway helps you deliver AI securely, cost-effectively, and at scale.\n\n**[Get started](#)** | **[View architecture guide](#)** | **[Try the demo](#)**","src/content/blog/secure-scalable-ai-inference-for-kubernetes.mdx","c20e698069f1e327","secure-scalable-ai-inference-for-kubernetes.mdx","token-alerting",{"id":444,"data":446,"body":23,"filePath":449,"digest":450,"rendered":451,"legacyId":457},{"title":447,"date":448,"description":22},"How to Set Up Alerting on Token Overages",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/token-alerting.md","d8cc45b2c11226ab",{"html":27,"metadata":452},{"headings":453,"localImagePaths":454,"remoteImagePaths":455,"frontmatter":446,"imagePaths":456},[],[],[],[],"token-alerting.md","token-cost-prometheus",{"id":458,"data":460,"body":23,"filePath":463,"digest":464,"rendered":465,"legacyId":471},{"title":461,"date":462,"description":22},"Track and Visualize Token Costs with Prometheus + Grafana",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/token-cost-prometheus.md","7c15dd63a183db73",{"html":27,"metadata":466},{"headings":467,"localImagePaths":468,"remoteImagePaths":469,"frontmatter":460,"imagePaths":470},[],[],[],[],"token-cost-prometheus.md","top-envoy-filters",{"id":472,"data":474,"body":23,"filePath":477,"digest":478,"rendered":479,"legacyId":485},{"title":475,"date":476,"description":22},"7 Envoy Filters Every AI Gateway Should Use",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/top-envoy-filters.md","8ee7d4e050e9e36b",{"html":27,"metadata":480},{"headings":481,"localImagePaths":482,"remoteImagePaths":483,"frontmatter":474,"imagePaths":484},[],[],[],[],"top-envoy-filters.md","usage-tier-enforcement",{"id":486,"data":488,"body":23,"filePath":491,"digest":492,"rendered":493,"legacyId":499},{"title":489,"date":490,"description":22},"How to Enforce Usage Tiers for AI Services",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/usage-tier-enforcement.md","c3a62156444b55ca",{"html":27,"metadata":494},{"headings":495,"localImagePaths":496,"remoteImagePaths":497,"frontmatter":488,"imagePaths":498},[],[],[],[],"usage-tier-enforcement.md","wasm-prompt-sanitization",{"id":500,"data":502,"body":23,"filePath":505,"digest":506,"rendered":507,"legacyId":513},{"title":503,"date":504,"description":22},"Using Wasm in Envoy AI Gateway to Sanitize Prompts",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/wasm-prompt-sanitization.md","6136eebeb8302392",{"html":27,"metadata":508},{"headings":509,"localImagePaths":510,"remoteImagePaths":511,"frontmatter":502,"imagePaths":512},[],[],[],[],"wasm-prompt-sanitization.md","tutorial-02-track-and-limit-llm-token-usage-in-langchain-with-envoy-ai-gateway",{"id":514,"data":516,"body":520,"filePath":521,"digest":522,"legacyId":523,"deferredRender":16},{"title":517,"date":518,"description":519},"Track and Limit LLM Token Usage in LangChain with Envoy AI Gateway",["Date","2025-05-28T00:00:00.000Z"],"Use Envoy AI Gateway to enforce token-aware rate limits for LangChain applications.","# Track and Limit LLM Token Usage in LangChain with Envoy AI Gateway\n\nLangChain enables chaining prompts and agents—but that can burn through tokens fast. Use Envoy AI Gateway to:\n\n- Count tokens per request\n- Enforce daily or session-based token quotas\n- Log and block overuse in real-time\n\n## What You'll Learn\n\n- How to inspect prompt size and response tokens\n- How to attach a token budget to a user or agent ID\n- How to build dashboards using Prometheus and Grafana\n\n## Sample Token Budget Policy (Conceptual)\n```yaml\ntokenBudgets:\n  - user: user123\n    maxTokens: 10000\n    period: day\n```\n\n**Ideal for copilots, assistants, and research workloads where token budgets are critical.**","src/content/blog/tutorial-02-track-and-limit-llm-token-usage-in-langchain-with-envoy-ai-gateway.mdx","108258727391ff6f","tutorial-02-track-and-limit-llm-token-usage-in-langchain-with-envoy-ai-gateway.mdx","what-is-an-ai-gateway",{"id":524,"data":526,"body":530,"filePath":531,"digest":532,"rendered":533,"legacyId":557},{"title":527,"date":528,"description":529},"What Is an AI Gateway and Why You Need One",["Date","2025-05-28T00:00:00.000Z"],"AI Gateways are designed to secure, control, and observe access to LLMs and model APIs. Here's why your platform team needs one.","# What Is an AI Gateway and Why You Need One\n\nAPI gateways have long been essential for securing and managing service traffic in microservice architectures. But AI workloads introduce fundamentally new challenges—ones that traditional gateways were never designed to handle.\n\nThat's where **AI Gateways** come in.\n\n## Why AI Workloads Need a New Kind of Gateway\n\nLarge language models (LLMs) and generative AI APIs differ from traditional services in three key ways:\n\n- **They’re expensive** — API calls often consume tokens billed by request, word, or model compute time.\n- **They’re sensitive** — LLMs may return user-generated data, require privacy controls, or demand robust authentication.\n- **They’re unpredictable** — With prompt engineering, model chaining, and multiple downstream services, usage patterns are hard to enforce without policy control.\n\nAs platform teams begin to productize AI for internal and external use, these issues become operational bottlenecks—and security risks.\n\n## What Does an AI Gateway Do?\n\nAn **AI Gateway** sits between clients (apps, developers, services) and your AI model endpoints (like OpenAI, Claude, or Hugging Face TGI). It provides:\n\n- 🔐 **Authentication and Authorization**  \n  Enforce OIDC, mTLS, and role-based access control (RBAC) per route or tenant.\n\n- ⏳ **Rate Limiting and Token Budgeting**  \n  Limit not just requests per minute, but also total tokens, prompt sizes, or cost-based quotas.\n\n- 📊 **Prompt Observability**  \n  Track who is sending prompts, how many tokens are used, what latency or error rates occur, and what content is passing through.\n\n- 🔍 **Policy Enforcement and Governance**  \n  Sanitize prompts, redact PII, block abuse patterns, and ensure LLM use aligns with company policies.\n\n## How Is It Different from a Traditional API Gateway?\n\nHere’s a quick comparison:\n\n| Feature                         | API Gateway | AI Gateway |\n|--------------------------------|-------------|------------|\n| Token-based rate limiting      | ❌          | ✅         |\n| Prompt observability           | ❌          | ✅         |\n| LLM-specific usage policies    | ❌          | ✅         |\n| OpenAI/HuggingFace compatibility | ❌        | ✅         |\n| Native Kubernetes integration  | ⚠️ (varies) | ✅         |\n| Support for multi-tenant access| ⚠️          | ✅         |\n\n## Use Cases for an AI Gateway\n\n- **Control OpenAI access** for internal developers with SSO and usage quotas.\n- **Expose Hugging Face models** to front-end apps with fine-grained policies.\n- **Monitor LLM cost usage** and set alerts before budgets are exceeded.\n- **Prevent prompt injection** and ensure safe, compliant prompt handling.\n\n## Meet Envoy AI Gateway\n\n**Envoy AI Gateway** is an open-source, Kubernetes-native solution built on Envoy Proxy and the Gateway API. It was designed specifically for the challenges of production AI.\n\nYou can use it to:\n- Secure model APIs with zero-trust security\n- Control costs with token-aware rate limiting\n- Gain visibility into AI usage across teams and tenants\n\nWhether you're building a platform for AI copilots, chatbots, document understanding, or agent workflows—**an AI Gateway should be your first line of defense and control**.\n\n---\n\n**📘 Ready to Try It?**  \n👉 [Get started with Envoy AI Gateway](#)  \n👉 [See the full tutorial series](#)","src/content/blog/what-is-an-ai-gateway.md","633eebafd2c2b93b",{"html":534,"metadata":535},"\u003Ch1 id=\"what-is-an-ai-gateway-and-why-you-need-one\">What Is an AI Gateway and Why You Need One\u003C/h1>\n\u003Cp>API gateways have long been essential for securing and managing service traffic in microservice architectures. But AI workloads introduce fundamentally new challenges—ones that traditional gateways were never designed to handle.\u003C/p>\n\u003Cp>That’s where \u003Cstrong>AI Gateways\u003C/strong> come in.\u003C/p>\n\u003Ch2 id=\"why-ai-workloads-need-a-new-kind-of-gateway\">Why AI Workloads Need a New Kind of Gateway\u003C/h2>\n\u003Cp>Large language models (LLMs) and generative AI APIs differ from traditional services in three key ways:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>They’re expensive\u003C/strong> — API calls often consume tokens billed by request, word, or model compute time.\u003C/li>\n\u003Cli>\u003Cstrong>They’re sensitive\u003C/strong> — LLMs may return user-generated data, require privacy controls, or demand robust authentication.\u003C/li>\n\u003Cli>\u003Cstrong>They’re unpredictable\u003C/strong> — With prompt engineering, model chaining, and multiple downstream services, usage patterns are hard to enforce without policy control.\u003C/li>\n\u003C/ul>\n\u003Cp>As platform teams begin to productize AI for internal and external use, these issues become operational bottlenecks—and security risks.\u003C/p>\n\u003Ch2 id=\"what-does-an-ai-gateway-do\">What Does an AI Gateway Do?\u003C/h2>\n\u003Cp>An \u003Cstrong>AI Gateway\u003C/strong> sits between clients (apps, developers, services) and your AI model endpoints (like OpenAI, Claude, or Hugging Face TGI). It provides:\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Cp>🔐 \u003Cstrong>Authentication and Authorization\u003C/strong>\u003Cbr>\nEnforce OIDC, mTLS, and role-based access control (RBAC) per route or tenant.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>⏳ \u003Cstrong>Rate Limiting and Token Budgeting\u003C/strong>\u003Cbr>\nLimit not just requests per minute, but also total tokens, prompt sizes, or cost-based quotas.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>📊 \u003Cstrong>Prompt Observability\u003C/strong>\u003Cbr>\nTrack who is sending prompts, how many tokens are used, what latency or error rates occur, and what content is passing through.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>🔍 \u003Cstrong>Policy Enforcement and Governance\u003C/strong>\u003Cbr>\nSanitize prompts, redact PII, block abuse patterns, and ensure LLM use aligns with company policies.\u003C/p>\n\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"how-is-it-different-from-a-traditional-api-gateway\">How Is It Different from a Traditional API Gateway?\u003C/h2>\n\u003Cp>Here’s a quick comparison:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Feature\u003C/th>\u003Cth>API Gateway\u003C/th>\u003Cth>AI Gateway\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Token-based rate limiting\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Prompt observability\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>LLM-specific usage policies\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>OpenAI/HuggingFace compatibility\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Native Kubernetes integration\u003C/td>\u003Ctd>⚠️ (varies)\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Support for multi-tenant access\u003C/td>\u003Ctd>⚠️\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Ch2 id=\"use-cases-for-an-ai-gateway\">Use Cases for an AI Gateway\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Control OpenAI access\u003C/strong> for internal developers with SSO and usage quotas.\u003C/li>\n\u003Cli>\u003Cstrong>Expose Hugging Face models\u003C/strong> to front-end apps with fine-grained policies.\u003C/li>\n\u003Cli>\u003Cstrong>Monitor LLM cost usage\u003C/strong> and set alerts before budgets are exceeded.\u003C/li>\n\u003Cli>\u003Cstrong>Prevent prompt injection\u003C/strong> and ensure safe, compliant prompt handling.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"meet-envoy-ai-gateway\">Meet Envoy AI Gateway\u003C/h2>\n\u003Cp>\u003Cstrong>Envoy AI Gateway\u003C/strong> is an open-source, Kubernetes-native solution built on Envoy Proxy and the Gateway API. It was designed specifically for the challenges of production AI.\u003C/p>\n\u003Cp>You can use it to:\u003C/p>\n\u003Cul>\n\u003Cli>Secure model APIs with zero-trust security\u003C/li>\n\u003Cli>Control costs with token-aware rate limiting\u003C/li>\n\u003Cli>Gain visibility into AI usage across teams and tenants\u003C/li>\n\u003C/ul>\n\u003Cp>Whether you’re building a platform for AI copilots, chatbots, document understanding, or agent workflows—\u003Cstrong>an AI Gateway should be your first line of defense and control\u003C/strong>.\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cstrong>📘 Ready to Try It?\u003C/strong>\u003Cbr>\n👉 \u003Ca href=\"#\">Get started with Envoy AI Gateway\u003C/a>\u003Cbr>\n👉 \u003Ca href=\"#\">See the full tutorial series\u003C/a>\u003C/p>",{"headings":536,"localImagePaths":554,"remoteImagePaths":555,"frontmatter":526,"imagePaths":556},[537,539,542,545,548,551],{"depth":282,"slug":538,"text":527},"what-is-an-ai-gateway-and-why-you-need-one",{"depth":174,"slug":540,"text":541},"why-ai-workloads-need-a-new-kind-of-gateway","Why AI Workloads Need a New Kind of Gateway",{"depth":174,"slug":543,"text":544},"what-does-an-ai-gateway-do","What Does an AI Gateway Do?",{"depth":174,"slug":546,"text":547},"how-is-it-different-from-a-traditional-api-gateway","How Is It Different from a Traditional API Gateway?",{"depth":174,"slug":549,"text":550},"use-cases-for-an-ai-gateway","Use Cases for an AI Gateway",{"depth":174,"slug":552,"text":553},"meet-envoy-ai-gateway","Meet Envoy AI Gateway",[],[],[],"what-is-an-ai-gateway.md","what-is-an-ai-gateway-and-why-do-you-need-one",{"id":558,"data":560,"body":564,"filePath":565,"digest":566,"legacyId":567,"deferredRender":16},{"title":561,"date":562,"description":563},"What Is an AI Gateway and Why Do You Need One?",["Date","2025-05-28T00:00:00.000Z"],"Understand the purpose of AI Gateways and why modern AI infrastructure needs policy-aware model traffic control.","# What Is an AI Gateway and Why Do You Need One?\n\nYou’ve secured your microservices with ingress controllers. You’ve adopted API gateways for external traffic. But what about your AI models?\n\nTraditional gateways weren't built for:\n\n- Prompt payloads that consume tokens and dollars\n- Models that need usage controls across tenants\n- Observability of LLM inputs and outputs\n\n## Enter the AI Gateway\nAn **AI Gateway** sits in front of model endpoints—like OpenAI, Claude, or internal Hugging Face models—and provides:\n\n- 🔐 Authentication and authorization (OIDC, mTLS)\n- ⏱️ Rate limiting and token budgets\n- 📊 Prompt observability and usage tracking\n- 📜 Policy enforcement and traffic governance\n\n## Do You Need One?\nYou do, if:\n\n- You have multiple teams accessing shared LLMs\n- You’re worried about API abuse, quota blowouts, or compliance\n- You want to scale your AI platform with confidence\n\n**Envoy AI Gateway** was built for this exact use case.\n\n**[Explore use cases](#)** | **[Read the Getting Started guide](#)** | **[Compare AI Gateways](#)**","src/content/blog/what-is-an-ai-gateway-and-why-do-you-need-one.mdx","eb8e3798b135ff9f","what-is-an-ai-gateway-and-why-do-you-need-one.mdx","tutorial-01-enforce-oidc-auth-for-openai-with-envoy-ai-gateway",{"id":568,"data":570,"body":574,"filePath":575,"digest":576,"legacyId":577,"deferredRender":16},{"title":571,"date":572,"description":573},"Authenticate OpenAI Traffic in Kubernetes with OIDC and Envoy AI Gateway",["Date","2025-05-28T00:00:00.000Z"],"Protect OpenAI access with OIDC-based authentication via Envoy AI Gateway in Kubernetes.","# Authenticate OpenAI Traffic in Kubernetes with OIDC and Envoy AI Gateway\n\nUncontrolled access to OpenAI can lead to shadow usage, security gaps, and surprise bills. By enforcing OIDC via Envoy AI Gateway, you can ensure that only authenticated and authorized users are able to access LLM endpoints.\n\n## What You'll Learn\n\n- How to configure Envoy AI Gateway with OIDC (e.g., Auth0, Okta, Google)\n- How to intercept OpenAI requests and enforce login\n- How to forward JWT claims for audit and policy control\n\n## Use Case\nYou want your internal dev portal to require SSO login before using GPT-based tools. You also want to:\n\n- Apply tenant-based rate limits\n- Track usage per authenticated user\n\n## Sample Policy Snippet\n```yaml\nauthentication:\n  - provider:\n      name: oidc-auth\n      issuer: https://auth.myorg.com\n      clientId: envoy-gateway\n      jwksUri: https://auth.myorg.com/.well-known/jwks.json\n```\n\n**Secure model access, maintain audit trails, and support zero trust architecture.**","src/content/blog/tutorial-01-enforce-oidc-auth-for-openai-with-envoy-ai-gateway.mdx","f0895a4576c339c1","tutorial-01-enforce-oidc-auth-for-openai-with-envoy-ai-gateway.mdx","tutorial-03-protect-internal-model-apis-seldon-bentoml-with-mtls-in-envoy-gateway",{"id":578,"data":580,"body":584,"filePath":585,"digest":586,"legacyId":587,"deferredRender":16},{"title":581,"date":582,"description":583},"Protect Internal Model APIs (Seldon, BentoML) with mTLS in Envoy AI Gateway",["Date","2025-05-28T00:00:00.000Z"],"Use mTLS in Envoy AI Gateway to secure internal ML model APIs and enforce zero-trust access.","# Protect Internal Model APIs (Seldon, BentoML) with mTLS in Envoy AI Gateway\n\nServing internal models in Kubernetes—via Seldon, BentoML, or Ray Serve—requires more than just network policies. Use Envoy AI Gateway to:\n\n- Enforce client identity with mTLS\n- Encrypt model traffic end-to-end\n- Prevent unauthorized service access\n\n## What You'll Learn\n\n- How to generate and sign client certificates\n- How to enable mTLS on Gateway routes\n- How to rotate certificates securely\n\n## Sample TLS Policy Snippet\n```yaml\ntls:\n  mode: MUTUAL\n  clientCertificateAuthorities:\n    - secretRef: seldon-ca-secret\n```\n\n**Raise the bar on internal security—without custom firewall rules or external proxies.**","src/content/blog/tutorial-03-protect-internal-model-apis-seldon-bentoml-with-mtls-in-envoy-gateway.mdx","738493786059479c","tutorial-03-protect-internal-model-apis-seldon-bentoml-with-mtls-in-envoy-gateway.mdx","track-token-costs-prometheus-grafana",{"id":588,"data":590,"body":594,"filePath":595,"digest":596,"rendered":597,"legacyId":625},{"title":591,"date":592,"description":593},"Track and Visualize Token Costs with Prometheus and Grafana",["Date","2025-05-28T00:00:00.000Z"],"Learn how to monitor LLM token usage and inference costs using Envoy AI Gateway with Prometheus and Grafana dashboards.","# Track and Visualize Token Costs with Prometheus and Grafana\n\nAs your usage of large language models (LLMs) grows, so do your costs. But tracking API billing from providers like OpenAI or Anthropic isn’t straightforward—especially when multiple teams, apps, or services are sharing access.\n\nWith **Envoy AI Gateway**, you can expose fine-grained **metrics for token usage, request rates, and response patterns**, and then visualize them using Prometheus and Grafana.\n\nThis post walks you through setting up a full stack for LLM cost observability.\n\n---\n\n## Why You Need Token-Level Visibility\n\nEven small prompts can add up quickly. Without visibility into usage patterns, platform teams often face:\n\n- Inability to forecast monthly API spend\n- Difficulty attributing costs to teams or services\n- No alerting before budget thresholds are hit\n- No insight into abnormal spikes or regressions\n\nBy integrating Envoy AI Gateway with Prometheus and Grafana, you can answer questions like:\n\n- Who’s using the most tokens?\n- Which apps are driving the highest cost?\n- When did a sudden spike begin?\n- Are latency and errors affecting certain tenants?\n\n---\n\n## Architecture Overview\n\n```\n[ App / CLI / Service ]\n        |\n   [ Envoy AI Gateway ]\n        |\n   ↙︎         ↘︎\nPrometheus   →  OpenAI / Claude / LLM\n     |\n  Grafana Dashboards\n```\n\n---\n\n## Step 1: Expose Metrics from Envoy AI Gateway\n\nEnvoy exposes metrics in a Prometheus-compatible format. When deploying Envoy AI Gateway:\n\n- Enable Prometheus scraping on the gateway’s metrics port\n- Export custom metrics for:\n  - `llm_tokens_in`\n  - `llm_tokens_out`\n  - `llm_total_cost_usd`\n  - `llm_requests_total`\n  - `llm_errors_total`\n  - `llm_latency_seconds`\n\nEach metric should be labeled by:\n- Tenant / team\n- Endpoint\n- Model\n- HTTP response code\n\n---\n\n## Step 2: Configure Prometheus\n\nIn your Prometheus config:\n\n```yaml\nscrape_configs:\n  - job_name: 'envoy-ai-gateway'\n    static_configs:\n      - targets: ['envoy-gateway-service:9102']\n```\n\nEnsure the port matches your gateway’s `stats` listener. You can also use Kubernetes service discovery for dynamic environments.\n\n---\n\n## Step 3: Create Grafana Dashboards\n\nUse Grafana to create dashboards that include:\n\n- 🔢 **Tokens used per day / week / month**\n- 📊 **Top consumers by team or service**\n- 💵 **Estimated cost (tokens × $/token)**\n- ⏱️ **Latency percentiles and errors**\n- 🚨 **Alerts for quota breaches or spikes**\n\nSample query to estimate token cost:\n\n```promql\nsum by (team) (llm_tokens_in + llm_tokens_out) * 0.00002\n```\n\nAdjust the multiplier for your OpenAI or Claude pricing.\n\n---\n\n## Bonus: Alerts and Budgets\n\nSet up alert rules in Prometheus to detect:\n\n- Unusual spikes in token usage\n- Daily budget exceeded per tenant\n- Missing metrics (e.g. exporter crash)\n\n---\n\n## Outcome\n\nWith this stack in place, you’ll gain:\n\n- Cost attribution for LLM usage\n- Predictive usage insights across environments\n- Alerts before overruns or service disruptions\n- Transparency for internal chargeback models\n\n---\n\n📈 **Get visibility into your AI costs today**  \n📘 [See full metrics and dashboard examples](#)  \n🚀 [Deploy Envoy AI Gateway + Prometheus stack](#)","src/content/blog/track-token-costs-prometheus-grafana.md","2c5e7559f9abb931",{"html":598,"metadata":599},"\u003Ch1 id=\"track-and-visualize-token-costs-with-prometheus-and-grafana\">Track and Visualize Token Costs with Prometheus and Grafana\u003C/h1>\n\u003Cp>As your usage of large language models (LLMs) grows, so do your costs. But tracking API billing from providers like OpenAI or Anthropic isn’t straightforward—especially when multiple teams, apps, or services are sharing access.\u003C/p>\n\u003Cp>With \u003Cstrong>Envoy AI Gateway\u003C/strong>, you can expose fine-grained \u003Cstrong>metrics for token usage, request rates, and response patterns\u003C/strong>, and then visualize them using Prometheus and Grafana.\u003C/p>\n\u003Cp>This post walks you through setting up a full stack for LLM cost observability.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"why-you-need-token-level-visibility\">Why You Need Token-Level Visibility\u003C/h2>\n\u003Cp>Even small prompts can add up quickly. Without visibility into usage patterns, platform teams often face:\u003C/p>\n\u003Cul>\n\u003Cli>Inability to forecast monthly API spend\u003C/li>\n\u003Cli>Difficulty attributing costs to teams or services\u003C/li>\n\u003Cli>No alerting before budget thresholds are hit\u003C/li>\n\u003Cli>No insight into abnormal spikes or regressions\u003C/li>\n\u003C/ul>\n\u003Cp>By integrating Envoy AI Gateway with Prometheus and Grafana, you can answer questions like:\u003C/p>\n\u003Cul>\n\u003Cli>Who’s using the most tokens?\u003C/li>\n\u003Cli>Which apps are driving the highest cost?\u003C/li>\n\u003Cli>When did a sudden spike begin?\u003C/li>\n\u003Cli>Are latency and errors affecting certain tenants?\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Ch2 id=\"architecture-overview\">Architecture Overview\u003C/h2>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>[ App / CLI / Service ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>        |\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>   [ Envoy AI Gateway ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>        |\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>   ↙︎         ↘︎\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Prometheus   →  OpenAI / Claude / LLM\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>     |\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>  Grafana Dashboards\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Chr>\n\u003Ch2 id=\"step-1-expose-metrics-from-envoy-ai-gateway\">Step 1: Expose Metrics from Envoy AI Gateway\u003C/h2>\n\u003Cp>Envoy exposes metrics in a Prometheus-compatible format. When deploying Envoy AI Gateway:\u003C/p>\n\u003Cul>\n\u003Cli>Enable Prometheus scraping on the gateway’s metrics port\u003C/li>\n\u003Cli>Export custom metrics for:\n\u003Cul>\n\u003Cli>\u003Ccode>llm_tokens_in\u003C/code>\u003C/li>\n\u003Cli>\u003Ccode>llm_tokens_out\u003C/code>\u003C/li>\n\u003Cli>\u003Ccode>llm_total_cost_usd\u003C/code>\u003C/li>\n\u003Cli>\u003Ccode>llm_requests_total\u003C/code>\u003C/li>\n\u003Cli>\u003Ccode>llm_errors_total\u003C/code>\u003C/li>\n\u003Cli>\u003Ccode>llm_latency_seconds\u003C/code>\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Cp>Each metric should be labeled by:\u003C/p>\n\u003Cul>\n\u003Cli>Tenant / team\u003C/li>\n\u003Cli>Endpoint\u003C/li>\n\u003Cli>Model\u003C/li>\n\u003Cli>HTTP response code\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Ch2 id=\"step-2-configure-prometheus\">Step 2: Configure Prometheus\u003C/h2>\n\u003Cp>In your Prometheus config:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"yaml\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">scrape_configs\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  - \u003C/span>\u003Cspan style=\"color:#85E89D\">job_name\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">'envoy-ai-gateway'\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    static_configs\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      - \u003C/span>\u003Cspan style=\"color:#85E89D\">targets\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003Cspan style=\"color:#9ECBFF\">'envoy-gateway-service:9102'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">]\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Ensure the port matches your gateway’s \u003Ccode>stats\u003C/code> listener. You can also use Kubernetes service discovery for dynamic environments.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"step-3-create-grafana-dashboards\">Step 3: Create Grafana Dashboards\u003C/h2>\n\u003Cp>Use Grafana to create dashboards that include:\u003C/p>\n\u003Cul>\n\u003Cli>🔢 \u003Cstrong>Tokens used per day / week / month\u003C/strong>\u003C/li>\n\u003Cli>📊 \u003Cstrong>Top consumers by team or service\u003C/strong>\u003C/li>\n\u003Cli>💵 \u003Cstrong>Estimated cost (tokens × $/token)\u003C/strong>\u003C/li>\n\u003Cli>⏱️ \u003Cstrong>Latency percentiles and errors\u003C/strong>\u003C/li>\n\u003Cli>🚨 \u003Cstrong>Alerts for quota breaches or spikes\u003C/strong>\u003C/li>\n\u003C/ul>\n\u003Cp>Sample query to estimate token cost:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>sum by (team) (llm_tokens_in + llm_tokens_out) * 0.00002\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Adjust the multiplier for your OpenAI or Claude pricing.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"bonus-alerts-and-budgets\">Bonus: Alerts and Budgets\u003C/h2>\n\u003Cp>Set up alert rules in Prometheus to detect:\u003C/p>\n\u003Cul>\n\u003Cli>Unusual spikes in token usage\u003C/li>\n\u003Cli>Daily budget exceeded per tenant\u003C/li>\n\u003Cli>Missing metrics (e.g. exporter crash)\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Ch2 id=\"outcome\">Outcome\u003C/h2>\n\u003Cp>With this stack in place, you’ll gain:\u003C/p>\n\u003Cul>\n\u003Cli>Cost attribution for LLM usage\u003C/li>\n\u003Cli>Predictive usage insights across environments\u003C/li>\n\u003Cli>Alerts before overruns or service disruptions\u003C/li>\n\u003Cli>Transparency for internal chargeback models\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Cp>📈 \u003Cstrong>Get visibility into your AI costs today\u003C/strong>\u003Cbr>\n📘 \u003Ca href=\"#\">See full metrics and dashboard examples\u003C/a>\u003Cbr>\n🚀 \u003Ca href=\"#\">Deploy Envoy AI Gateway + Prometheus stack\u003C/a>\u003C/p>",{"headings":600,"localImagePaths":622,"remoteImagePaths":623,"frontmatter":590,"imagePaths":624},[601,603,606,607,610,613,616,619],{"depth":282,"slug":602,"text":591},"track-and-visualize-token-costs-with-prometheus-and-grafana",{"depth":174,"slug":604,"text":605},"why-you-need-token-level-visibility","Why You Need Token-Level Visibility",{"depth":174,"slug":379,"text":380},{"depth":174,"slug":608,"text":609},"step-1-expose-metrics-from-envoy-ai-gateway","Step 1: Expose Metrics from Envoy AI Gateway",{"depth":174,"slug":611,"text":612},"step-2-configure-prometheus","Step 2: Configure Prometheus",{"depth":174,"slug":614,"text":615},"step-3-create-grafana-dashboards","Step 3: Create Grafana Dashboards",{"depth":174,"slug":617,"text":618},"bonus-alerts-and-budgets","Bonus: Alerts and Budgets",{"depth":174,"slug":620,"text":621},"outcome","Outcome",[],[],[],"track-token-costs-prometheus-grafana.md","prompt-observability-llm-apps",{"id":626,"data":628,"body":632,"filePath":633,"digest":634,"rendered":635,"legacyId":663},{"title":629,"date":630,"description":631},"Add Prompt Observability to Your LLM Apps with Envoy AI Gateway",["Date","2025-05-28T00:00:00.000Z"],"Learn how to observe prompt inputs, outputs, latency, token counts, and more in LLM applications using Envoy AI Gateway.","# Add Prompt Observability to Your LLM Apps with Envoy AI Gateway\n\nAs LLM usage spreads across your organization, observability becomes essential—not just for infrastructure, but for the prompts and responses themselves.\n\nYou need visibility into:\n\n- Who is sending prompts\n- What’s being sent and returned\n- How much it costs\n- How fast or error-prone the experience is\n\nTraditional monitoring tools can't answer these questions. But **Envoy AI Gateway** can.\n\nIn this guide, you’ll learn how to capture detailed prompt observability from the edge of your infrastructure—all without changing your app code.\n\n---\n\n## Why Prompt-Level Observability Matters\n\nLLMs are:\n- Expensive (billed per token)\n- Sensitive (may contain PII)\n- Complex (chained requests, dynamic prompts)\n\nWithout observability at the gateway, you’re flying blind.\n\nPrompt-level observability helps you:\n- Attribute usage by user, team, or app\n- Detect anomalies and usage spikes\n- Trace issues back to problematic inputs\n- Optimize latency and prompt quality\n- Ensure privacy and policy compliance\n\n---\n\n## Observability Features in Envoy AI Gateway\n\nEnvoy AI Gateway exposes rich telemetry for LLM traffic:\n\n- **Request metadata**  \n  Method, path, headers, source IP\n\n- **JWT claims / identity context**  \n  Who made the request\n\n- **Prompt and response body metadata**  \n  (lengths, token counts—not full text unless configured)\n\n- **Latency and failure rates**\n\n- **Token usage and cost estimation**\n\nAll metrics are emitted in Prometheus/OpenTelemetry-compatible formats.\n\n---\n\n## Example Metrics\n\n```text\nllm_prompt_tokens{team=\"r&d\"} 45210\nllm_completion_tokens{user=\"alice\"} 38491\nllm_latency_seconds{model=\"gpt-4\"} 1.94\nllm_errors_total{route=\"/v1/chat/completions\"} 3\n```\n\n---\n\n## Example Tracing View\n\nUsing OpenTelemetry + Jaeger or Tempo, you can visualize:\n\n```\nTrace: POST /v1/chat/completions\n├── Auth: alice@company.com\n├── Team: R&D\n├── Tokens In: 423\n├── Tokens Out: 931\n├── Model: gpt-4\n├── Latency: 2.4s\n└── Status: 200 OK\n```\n\n---\n\n## Privacy and Redaction\n\nYou can configure Envoy AI Gateway to:\n- **Redact prompt/response bodies** entirely\n- **Capture prompt structure but not values**\n- **Hash or tokenize sensitive strings**\n\nPrompt observability is **safe by default**, but flexible enough for audit and debugging.\n\n---\n\n## Bonus: Logging and Analytics\n\nLogs can be shipped to:\n- ELK stack\n- Datadog\n- FluentBit → BigQuery or S3\n\nUse them for:\n- Usage audits\n- Incident investigations\n- Prompt quality reviews\n\n---\n\n## Conclusion\n\nPrompt observability is no longer a nice-to-have—it’s a must-have for anyone deploying LLMs at scale.\n\nEnvoy AI Gateway gives you this capability at the edge, without needing to modify every app or prompt call.\n\n---\n\n🔍 **Get deep visibility into your AI apps**  \n📘 [See prompt observability examples](#)  \n🚀 [Add Envoy AI Gateway to your stack](#)","src/content/blog/prompt-observability-llm-apps.md","54e3f00d98c96bd9",{"html":636,"metadata":637},"\u003Ch1 id=\"add-prompt-observability-to-your-llm-apps-with-envoy-ai-gateway\">Add Prompt Observability to Your LLM Apps with Envoy AI Gateway\u003C/h1>\n\u003Cp>As LLM usage spreads across your organization, observability becomes essential—not just for infrastructure, but for the prompts and responses themselves.\u003C/p>\n\u003Cp>You need visibility into:\u003C/p>\n\u003Cul>\n\u003Cli>Who is sending prompts\u003C/li>\n\u003Cli>What’s being sent and returned\u003C/li>\n\u003Cli>How much it costs\u003C/li>\n\u003Cli>How fast or error-prone the experience is\u003C/li>\n\u003C/ul>\n\u003Cp>Traditional monitoring tools can’t answer these questions. But \u003Cstrong>Envoy AI Gateway\u003C/strong> can.\u003C/p>\n\u003Cp>In this guide, you’ll learn how to capture detailed prompt observability from the edge of your infrastructure—all without changing your app code.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"why-prompt-level-observability-matters\">Why Prompt-Level Observability Matters\u003C/h2>\n\u003Cp>LLMs are:\u003C/p>\n\u003Cul>\n\u003Cli>Expensive (billed per token)\u003C/li>\n\u003Cli>Sensitive (may contain PII)\u003C/li>\n\u003Cli>Complex (chained requests, dynamic prompts)\u003C/li>\n\u003C/ul>\n\u003Cp>Without observability at the gateway, you’re flying blind.\u003C/p>\n\u003Cp>Prompt-level observability helps you:\u003C/p>\n\u003Cul>\n\u003Cli>Attribute usage by user, team, or app\u003C/li>\n\u003Cli>Detect anomalies and usage spikes\u003C/li>\n\u003Cli>Trace issues back to problematic inputs\u003C/li>\n\u003Cli>Optimize latency and prompt quality\u003C/li>\n\u003Cli>Ensure privacy and policy compliance\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Ch2 id=\"observability-features-in-envoy-ai-gateway\">Observability Features in Envoy AI Gateway\u003C/h2>\n\u003Cp>Envoy AI Gateway exposes rich telemetry for LLM traffic:\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Request metadata\u003C/strong>\u003Cbr>\nMethod, path, headers, source IP\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>JWT claims / identity context\u003C/strong>\u003Cbr>\nWho made the request\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Prompt and response body metadata\u003C/strong>\u003Cbr>\n(lengths, token counts—not full text unless configured)\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Latency and failure rates\u003C/strong>\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Token usage and cost estimation\u003C/strong>\u003C/p>\n\u003C/li>\n\u003C/ul>\n\u003Cp>All metrics are emitted in Prometheus/OpenTelemetry-compatible formats.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"example-metrics\">Example Metrics\u003C/h2>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"text\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>llm_prompt_tokens{team=\"r&#x26;d\"} 45210\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>llm_completion_tokens{user=\"alice\"} 38491\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>llm_latency_seconds{model=\"gpt-4\"} 1.94\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>llm_errors_total{route=\"/v1/chat/completions\"} 3\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Chr>\n\u003Ch2 id=\"example-tracing-view\">Example Tracing View\u003C/h2>\n\u003Cp>Using OpenTelemetry + Jaeger or Tempo, you can visualize:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Trace: POST /v1/chat/completions\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>├── Auth: alice@company.com\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>├── Team: R&#x26;D\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>├── Tokens In: 423\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>├── Tokens Out: 931\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>├── Model: gpt-4\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>├── Latency: 2.4s\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>└── Status: 200 OK\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Chr>\n\u003Ch2 id=\"privacy-and-redaction\">Privacy and Redaction\u003C/h2>\n\u003Cp>You can configure Envoy AI Gateway to:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Redact prompt/response bodies\u003C/strong> entirely\u003C/li>\n\u003Cli>\u003Cstrong>Capture prompt structure but not values\u003C/strong>\u003C/li>\n\u003Cli>\u003Cstrong>Hash or tokenize sensitive strings\u003C/strong>\u003C/li>\n\u003C/ul>\n\u003Cp>Prompt observability is \u003Cstrong>safe by default\u003C/strong>, but flexible enough for audit and debugging.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"bonus-logging-and-analytics\">Bonus: Logging and Analytics\u003C/h2>\n\u003Cp>Logs can be shipped to:\u003C/p>\n\u003Cul>\n\u003Cli>ELK stack\u003C/li>\n\u003Cli>Datadog\u003C/li>\n\u003Cli>FluentBit → BigQuery or S3\u003C/li>\n\u003C/ul>\n\u003Cp>Use them for:\u003C/p>\n\u003Cul>\n\u003Cli>Usage audits\u003C/li>\n\u003Cli>Incident investigations\u003C/li>\n\u003Cli>Prompt quality reviews\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>Prompt observability is no longer a nice-to-have—it’s a must-have for anyone deploying LLMs at scale.\u003C/p>\n\u003Cp>Envoy AI Gateway gives you this capability at the edge, without needing to modify every app or prompt call.\u003C/p>\n\u003Chr>\n\u003Cp>🔍 \u003Cstrong>Get deep visibility into your AI apps\u003C/strong>\u003Cbr>\n📘 \u003Ca href=\"#\">See prompt observability examples\u003C/a>\u003Cbr>\n🚀 \u003Ca href=\"#\">Add Envoy AI Gateway to your stack\u003C/a>\u003C/p>",{"headings":638,"localImagePaths":660,"remoteImagePaths":661,"frontmatter":628,"imagePaths":662},[639,641,644,647,650,653,656,659],{"depth":282,"slug":640,"text":629},"add-prompt-observability-to-your-llm-apps-with-envoy-ai-gateway",{"depth":174,"slug":642,"text":643},"why-prompt-level-observability-matters","Why Prompt-Level Observability Matters",{"depth":174,"slug":645,"text":646},"observability-features-in-envoy-ai-gateway","Observability Features in Envoy AI Gateway",{"depth":174,"slug":648,"text":649},"example-metrics","Example Metrics",{"depth":174,"slug":651,"text":652},"example-tracing-view","Example Tracing View",{"depth":174,"slug":654,"text":655},"privacy-and-redaction","Privacy and Redaction",{"depth":174,"slug":657,"text":658},"bonus-logging-and-analytics","Bonus: Logging and Analytics",{"depth":174,"slug":312,"text":313},[],[],[],"prompt-observability-llm-apps.md"]
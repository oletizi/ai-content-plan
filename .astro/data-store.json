[["Map",1,2,7,8],"meta::meta",["Map",3,4,5,6],"astro-version","5.8.0","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://envoy-ai-gateway.io\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false},\"legacy\":{\"collections\":false}}","blog",["Map",9,10,45,46,62,63,76,77,90,91,104,105,118,119,132,133,146,147,160,161,174,175,188,189,215,216,229,230,243,244,257,258,268,269,282,283,296,297,310,311,324,325,338,339,352,353,366,367,380,381,394,395,404,405,412,413,426,427,440,441,454,455,468,469,482,483,30,492,525,526,535,536,545,546],"ai-gateway-vs-api-gateway",{"id":9,"data":11,"body":15,"filePath":16,"digest":17,"rendered":18,"legacyId":44},{"title":12,"date":13,"description":14},"AI Gateway vs API Gateway: What’s the Difference and Why It Matters",["Date","2025-05-28T00:00:00.000Z"],"AI Gateways are built to solve challenges that traditional API gateways can't—like token-based rate limits, model cost control, and LLM observability. Here's how they compare.","# AI Gateway vs API Gateway: What’s the Difference and Why It Matters\n\nAPI gateways have been a pillar of microservices architecture—managing traffic, enforcing authentication, and providing routing and observability. But AI models and LLMs change the game.\n\nThe traffic patterns, risks, and requirements of AI are unlike anything traditional API gateways were designed for.\n\n## Why AI Traffic Is Different\n\nWhen serving or consuming AI models—like OpenAI, Hugging Face TGI, Claude, or custom LLMs—you face distinct challenges:\n\n- **Token-based billing** — Usage is measured in tokens, not just requests.\n- **High-cost payloads** — A single prompt can cost more than 1,000 API calls.\n- **Sensitive content** — Prompts and completions may include PII or proprietary information.\n- **Security risk** — LLMs can be exploited via prompt injection or abused by overuse.\n\nThese risks demand **a new kind of gateway**—purpose-built for AI.\n\n## What Is an AI Gateway?\n\nAn **AI Gateway** is a modern API gateway designed specifically for AI/ML workloads. It understands token usage, supports LLM-specific policy enforcement, and integrates with observability and security tooling.\n\nYou can think of it as:\n> **An API gateway with first-class support for inference, prompts, and AI-specific cost control.**\n\n## Side-by-Side Comparison\n\n| Capability                          | Traditional API Gateway | AI Gateway |\n|------------------------------------|--------------------------|------------|\n| Request rate limiting              | ✅                       | ✅         |\n| Token-based rate limiting          | ❌                       | ✅         |\n| Prompt/response size enforcement   | ❌                       | ✅         |\n| Auth via OIDC / mTLS               | ✅                       | ✅         |\n| LLM observability (token usage, latency) | ❌                | ✅         |\n| Policy enforcement per model route | ⚠️ Partial               | ✅         |\n| Prompt sanitization (Wasm)         | ❌                       | ✅         |\n| Cost-aware quota enforcement       | ❌                       | ✅         |\n| Support for multi-tenancy          | ⚠️ Varies                | ✅         |\n| Kubernetes Gateway API integration | ⚠️ Rare                  | ✅         |\n\n## When Should You Use an AI Gateway?\n\nYou need an AI Gateway if you’re:\n\n- Exposing model endpoints to internal devs or teams\n- Using OpenAI or Claude with SSO and RBAC\n- Deploying Hugging Face or custom models behind a load balancer\n- Concerned about inference costs, abuse, or auditability\n- Supporting multiple products or tenants that consume AI\n\n## The Bottom Line\n\nTraditional API gateways are not equipped to handle the unique access, observability, and control needs of AI workloads.\n\n**AI Gateways fill that gap.**\n\nThey bring policy control, cost governance, and secure access to a new class of APIs: generative, sensitive, and expensive.\n\n---\n\n**Meet Envoy AI Gateway**  \nA Kubernetes-native, open-source AI Gateway built on Envoy Proxy and the Gateway API.\n\n✅ Secure model access  \n✅ Enforce token-aware limits  \n✅ Observe prompt traffic and latency  \n✅ Govern AI usage per team, app, or tenant\n\n📘 [Learn more about Envoy AI Gateway](#)  \n🚀 [Try it now in your cluster](#)","src/content/blog/ai-gateway-vs-api-gateway.md","6bcdccf35f10c96b",{"html":19,"metadata":20},"\u003Ch1 id=\"ai-gateway-vs-api-gateway-whats-the-difference-and-why-it-matters\">AI Gateway vs API Gateway: What’s the Difference and Why It Matters\u003C/h1>\n\u003Cp>API gateways have been a pillar of microservices architecture—managing traffic, enforcing authentication, and providing routing and observability. But AI models and LLMs change the game.\u003C/p>\n\u003Cp>The traffic patterns, risks, and requirements of AI are unlike anything traditional API gateways were designed for.\u003C/p>\n\u003Ch2 id=\"why-ai-traffic-is-different\">Why AI Traffic Is Different\u003C/h2>\n\u003Cp>When serving or consuming AI models—like OpenAI, Hugging Face TGI, Claude, or custom LLMs—you face distinct challenges:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Token-based billing\u003C/strong> — Usage is measured in tokens, not just requests.\u003C/li>\n\u003Cli>\u003Cstrong>High-cost payloads\u003C/strong> — A single prompt can cost more than 1,000 API calls.\u003C/li>\n\u003Cli>\u003Cstrong>Sensitive content\u003C/strong> — Prompts and completions may include PII or proprietary information.\u003C/li>\n\u003Cli>\u003Cstrong>Security risk\u003C/strong> — LLMs can be exploited via prompt injection or abused by overuse.\u003C/li>\n\u003C/ul>\n\u003Cp>These risks demand \u003Cstrong>a new kind of gateway\u003C/strong>—purpose-built for AI.\u003C/p>\n\u003Ch2 id=\"what-is-an-ai-gateway\">What Is an AI Gateway?\u003C/h2>\n\u003Cp>An \u003Cstrong>AI Gateway\u003C/strong> is a modern API gateway designed specifically for AI/ML workloads. It understands token usage, supports LLM-specific policy enforcement, and integrates with observability and security tooling.\u003C/p>\n\u003Cp>You can think of it as:\u003C/p>\n\u003Cblockquote>\n\u003Cp>\u003Cstrong>An API gateway with first-class support for inference, prompts, and AI-specific cost control.\u003C/strong>\u003C/p>\n\u003C/blockquote>\n\u003Ch2 id=\"side-by-side-comparison\">Side-by-Side Comparison\u003C/h2>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Capability\u003C/th>\u003Cth>Traditional API Gateway\u003C/th>\u003Cth>AI Gateway\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Request rate limiting\u003C/td>\u003Ctd>✅\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Token-based rate limiting\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Prompt/response size enforcement\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Auth via OIDC / mTLS\u003C/td>\u003Ctd>✅\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>LLM observability (token usage, latency)\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Policy enforcement per model route\u003C/td>\u003Ctd>⚠️ Partial\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Prompt sanitization (Wasm)\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Cost-aware quota enforcement\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Support for multi-tenancy\u003C/td>\u003Ctd>⚠️ Varies\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Kubernetes Gateway API integration\u003C/td>\u003Ctd>⚠️ Rare\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Ch2 id=\"when-should-you-use-an-ai-gateway\">When Should You Use an AI Gateway?\u003C/h2>\n\u003Cp>You need an AI Gateway if you’re:\u003C/p>\n\u003Cul>\n\u003Cli>Exposing model endpoints to internal devs or teams\u003C/li>\n\u003Cli>Using OpenAI or Claude with SSO and RBAC\u003C/li>\n\u003Cli>Deploying Hugging Face or custom models behind a load balancer\u003C/li>\n\u003Cli>Concerned about inference costs, abuse, or auditability\u003C/li>\n\u003Cli>Supporting multiple products or tenants that consume AI\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"the-bottom-line\">The Bottom Line\u003C/h2>\n\u003Cp>Traditional API gateways are not equipped to handle the unique access, observability, and control needs of AI workloads.\u003C/p>\n\u003Cp>\u003Cstrong>AI Gateways fill that gap.\u003C/strong>\u003C/p>\n\u003Cp>They bring policy control, cost governance, and secure access to a new class of APIs: generative, sensitive, and expensive.\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cstrong>Meet Envoy AI Gateway\u003C/strong>\u003Cbr>\nA Kubernetes-native, open-source AI Gateway built on Envoy Proxy and the Gateway API.\u003C/p>\n\u003Cp>✅ Secure model access\u003Cbr>\n✅ Enforce token-aware limits\u003Cbr>\n✅ Observe prompt traffic and latency\u003Cbr>\n✅ Govern AI usage per team, app, or tenant\u003C/p>\n\u003Cp>📘 \u003Ca href=\"#\">Learn more about Envoy AI Gateway\u003C/a>\u003Cbr>\n🚀 \u003Ca href=\"#\">Try it now in your cluster\u003C/a>\u003C/p>",{"headings":21,"localImagePaths":41,"remoteImagePaths":42,"frontmatter":11,"imagePaths":43},[22,25,29,32,35,38],{"depth":23,"slug":24,"text":12},1,"ai-gateway-vs-api-gateway-whats-the-difference-and-why-it-matters",{"depth":26,"slug":27,"text":28},2,"why-ai-traffic-is-different","Why AI Traffic Is Different",{"depth":26,"slug":30,"text":31},"what-is-an-ai-gateway","What Is an AI Gateway?",{"depth":26,"slug":33,"text":34},"side-by-side-comparison","Side-by-Side Comparison",{"depth":26,"slug":36,"text":37},"when-should-you-use-an-ai-gateway","When Should You Use an AI Gateway?",{"depth":26,"slug":39,"text":40},"the-bottom-line","The Bottom Line",[],[],[],"ai-gateway-vs-api-gateway.md","ai-api-cost-governance",{"id":45,"data":47,"body":51,"filePath":52,"digest":53,"rendered":54,"legacyId":61},{"title":48,"date":49,"description":50},"AI API Cost Governance: Lessons from Internal Platforms",["Date","2025-05-28T00:00:00.000Z"],"This post is coming soon!","*This article is part of our 30-day content series for Envoy AI Gateway. Stay tuned!*","src/content/blog/ai-api-cost-governance.md","0f1c686ce820e1a0",{"html":55,"metadata":56},"\u003Cp>\u003Cem>This article is part of our 30-day content series for Envoy AI Gateway. Stay tuned!\u003C/em>\u003C/p>",{"headings":57,"localImagePaths":58,"remoteImagePaths":59,"frontmatter":47,"imagePaths":60},[],[],[],[],"ai-api-cost-governance.md","ai-gateway-roadmap",{"id":62,"data":64,"body":51,"filePath":67,"digest":68,"rendered":69,"legacyId":75},{"title":65,"date":66,"description":50},"AI Gateway Roadmap: What’s Next for LLM Traffic Control",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/ai-gateway-roadmap.md","3fed68b62aa19f0c",{"html":55,"metadata":70},{"headings":71,"localImagePaths":72,"remoteImagePaths":73,"frontmatter":64,"imagePaths":74},[],[],[],[],"ai-gateway-roadmap.md","ai-platform-challenges",{"id":76,"data":78,"body":51,"filePath":81,"digest":82,"rendered":83,"legacyId":89},{"title":79,"date":80,"description":50},"5 Problems Every AI Platform Team Faces with LLMs",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/ai-platform-challenges.md","1e3acdfea703d54c",{"html":55,"metadata":84},{"headings":85,"localImagePaths":86,"remoteImagePaths":87,"frontmatter":78,"imagePaths":88},[],[],[],[],"ai-platform-challenges.md","compare-ingress-controllers",{"id":90,"data":92,"body":51,"filePath":95,"digest":96,"rendered":97,"legacyId":103},{"title":93,"date":94,"description":50},"Compare: Envoy Gateway vs Kong vs NGINX for AI Ingress",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/compare-ingress-controllers.md","8d4dd474176dac71",{"html":55,"metadata":98},{"headings":99,"localImagePaths":100,"remoteImagePaths":101,"frontmatter":92,"imagePaths":102},[],[],[],[],"compare-ingress-controllers.md","fintech-case-study",{"id":104,"data":106,"body":51,"filePath":109,"digest":110,"rendered":111,"legacyId":117},{"title":107,"date":108,"description":50},"How a Fintech Platform Team Secured LLMs with Envoy AI Gateway",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/fintech-case-study.md","2e81b32e6a9f827e",{"html":55,"metadata":112},{"headings":113,"localImagePaths":114,"remoteImagePaths":115,"frontmatter":106,"imagePaths":116},[],[],[],[],"fintech-case-study.md","gateway-api-for-ai",{"id":118,"data":120,"body":51,"filePath":123,"digest":124,"rendered":125,"legacyId":131},{"title":121,"date":122,"description":50},"A Guide to the Kubernetes Gateway API for AI Traffic",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/gateway-api-for-ai.md","e7bdf51edf828f51",{"html":55,"metadata":126},{"headings":127,"localImagePaths":128,"remoteImagePaths":129,"frontmatter":120,"imagePaths":130},[],[],[],[],"gateway-api-for-ai.md","gitops-argo",{"id":132,"data":134,"body":51,"filePath":137,"digest":138,"rendered":139,"legacyId":145},{"title":135,"date":136,"description":50},"Deploying Envoy AI Gateway with GitOps and Argo CD",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/gitops-argo.md","91c27400f973efa7",{"html":55,"metadata":140},{"headings":141,"localImagePaths":142,"remoteImagePaths":143,"frontmatter":134,"imagePaths":144},[],[],[],[],"gitops-argo.md","cut-llm-costs",{"id":146,"data":148,"body":51,"filePath":151,"digest":152,"rendered":153,"legacyId":159},{"title":149,"date":150,"description":50},"How We Cut Our LLM Bill by 40% With Envoy Gateway",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/cut-llm-costs.md","8354fff407f1e7bd",{"html":55,"metadata":154},{"headings":155,"localImagePaths":156,"remoteImagePaths":157,"frontmatter":148,"imagePaths":158},[],[],[],[],"cut-llm-costs.md","gke-deploy-guide",{"id":160,"data":162,"body":51,"filePath":165,"digest":166,"rendered":167,"legacyId":173},{"title":163,"date":164,"description":50},"Deploy Envoy AI Gateway in GKE with Gateway API",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/gke-deploy-guide.md","93529ae19aebcb07",{"html":55,"metadata":168},{"headings":169,"localImagePaths":170,"remoteImagePaths":171,"frontmatter":162,"imagePaths":172},[],[],[],[],"gke-deploy-guide.md","huggingface-secure",{"id":174,"data":176,"body":51,"filePath":179,"digest":180,"rendered":181,"legacyId":187},{"title":177,"date":178,"description":50},"Secure Hugging Face TGI with Auth and Rate Limits",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/huggingface-secure.md","6079819c70c8b965",{"html":55,"metadata":182},{"headings":183,"localImagePaths":184,"remoteImagePaths":185,"frontmatter":176,"imagePaths":186},[],[],[],[],"huggingface-secure.md","introducing-envoy-ai-gateway",{"id":188,"data":190,"body":194,"filePath":195,"digest":196,"rendered":197,"legacyId":214},{"title":191,"date":192,"description":193},"Introducing Envoy AI Gateway: Secure, Scalable AI Inference for Kubernetes",["Date","2025-05-28T00:00:00.000Z"],"Envoy AI Gateway provides a secure, scalable, Kubernetes-native way to manage access to AI model endpoints.","AI models are powering everything from chatbots to copilots—but most teams are exposing them through insecure, unobservable, and expensive APIs. Envoy AI Gateway solves this by putting AI access behind a secure, policy-aware, and Kubernetes-native gateway.\n\n## The Problem\n\nProductionizing AI brings serious challenges:\n\n- No easy way to apply authentication or rate limits\n- Token sprawl and runaway inference costs\n- Lack of observability into model usage—or abuse\n\n## The Solution\n\n**Envoy AI Gateway** is built on Envoy Proxy and the Kubernetes Gateway API. It gives platform teams a modern, secure, and extensible way to manage AI inference traffic.\n\n### Key Benefits\n\n- ✅ Enforce mTLS, OIDC, and RBAC for LLM endpoints\n- ✅ Control usage with token-aware rate limits and quotas\n- ✅ Observe prompt traffic, latency, and errors\n- ✅ Native Kubernetes and GitOps support","src/content/blog/introducing-envoy-ai-gateway.md","10aecf8b22cd5d65",{"html":198,"metadata":199},"\u003Cp>AI models are powering everything from chatbots to copilots—but most teams are exposing them through insecure, unobservable, and expensive APIs. Envoy AI Gateway solves this by putting AI access behind a secure, policy-aware, and Kubernetes-native gateway.\u003C/p>\n\u003Ch2 id=\"the-problem\">The Problem\u003C/h2>\n\u003Cp>Productionizing AI brings serious challenges:\u003C/p>\n\u003Cul>\n\u003Cli>No easy way to apply authentication or rate limits\u003C/li>\n\u003Cli>Token sprawl and runaway inference costs\u003C/li>\n\u003Cli>Lack of observability into model usage—or abuse\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"the-solution\">The Solution\u003C/h2>\n\u003Cp>\u003Cstrong>Envoy AI Gateway\u003C/strong> is built on Envoy Proxy and the Kubernetes Gateway API. It gives platform teams a modern, secure, and extensible way to manage AI inference traffic.\u003C/p>\n\u003Ch3 id=\"key-benefits\">Key Benefits\u003C/h3>\n\u003Cul>\n\u003Cli>✅ Enforce mTLS, OIDC, and RBAC for LLM endpoints\u003C/li>\n\u003Cli>✅ Control usage with token-aware rate limits and quotas\u003C/li>\n\u003Cli>✅ Observe prompt traffic, latency, and errors\u003C/li>\n\u003Cli>✅ Native Kubernetes and GitOps support\u003C/li>\n\u003C/ul>",{"headings":200,"localImagePaths":211,"remoteImagePaths":212,"frontmatter":190,"imagePaths":213},[201,204,207],{"depth":26,"slug":202,"text":203},"the-problem","The Problem",{"depth":26,"slug":205,"text":206},"the-solution","The Solution",{"depth":208,"slug":209,"text":210},3,"key-benefits","Key Benefits",[],[],[],"introducing-envoy-ai-gateway.md","langchain-token-budgets",{"id":215,"data":217,"body":51,"filePath":220,"digest":221,"rendered":222,"legacyId":228},{"title":218,"date":219,"description":50},"Building Token Budgets for LangChain Agents",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/langchain-token-budgets.md","d8402b0a67bb964a",{"html":55,"metadata":223},{"headings":224,"localImagePaths":225,"remoteImagePaths":226,"frontmatter":217,"imagePaths":227},[],[],[],[],"langchain-token-budgets.md","langserve-fastapi",{"id":229,"data":231,"body":51,"filePath":234,"digest":235,"rendered":236,"legacyId":242},{"title":232,"date":233,"description":50},"Using Envoy Gateway with LangServe and FastAPI",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/langserve-fastapi.md","cd1fc71cf4d2f0f2",{"html":55,"metadata":237},{"headings":238,"localImagePaths":239,"remoteImagePaths":240,"frontmatter":231,"imagePaths":241},[],[],[],[],"langserve-fastapi.md","multi-tenant-inference",{"id":243,"data":245,"body":51,"filePath":248,"digest":249,"rendered":250,"legacyId":256},{"title":246,"date":247,"description":50},"Envoy Gateway for Multi-Tenant AI Inference",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/multi-tenant-inference.md","7988086e9c44d8ee",{"html":55,"metadata":251},{"headings":252,"localImagePaths":253,"remoteImagePaths":254,"frontmatter":245,"imagePaths":255},[],[],[],[],"multi-tenant-inference.md","how-to-rate-limit-openai-access-in-kubernetes-using-envoy-ai-gateway",{"id":257,"data":259,"body":263,"filePath":264,"digest":265,"legacyId":266,"deferredRender":267},{"title":260,"date":261,"description":262},"How to Rate Limit OpenAI Access in Kubernetes Using Envoy AI Gateway",["Date","2025-05-28T00:00:00.000Z"],"Step-by-step tutorial to apply rate limits to OpenAI's API using Envoy AI Gateway in Kubernetes.","# How to Rate Limit OpenAI Access in Kubernetes Using Envoy AI Gateway\n\nLLMs are powerful—and expensive. Without controls, a single integration can burn through your OpenAI quota in minutes.\n\n**Envoy AI Gateway** helps you:\n\n- Intercept and control requests to OpenAI\n- Apply per-user or per-service quotas\n- Track request and token usage\n\n## What You’ll Learn\n- How to deploy Envoy AI Gateway in your cluster\n- Define Gateway API resources with rate-limiting policies\n- Configure token budgets and enforce per-tenant usage\n- Monitor usage with built-in telemetry\n\n## Example Policy\n```yaml\nrateLimits:\n  - name: openai-limit\n    match:\n      path: /v1/chat/completions\n    requestsPerUnit: 100\n    unit: minute\n```\n\nThis allows you to safely expose OpenAI while keeping costs under control.\n\n**[Follow the full tutorial](#)** | **[Join the community](#)** | **[Deploy secure AI inference](#)**","src/content/blog/how-to-rate-limit-openai-access-in-kubernetes-using-envoy-ai-gateway.mdx","0db25eb8e9166e03","how-to-rate-limit-openai-access-in-kubernetes-using-envoy-ai-gateway.mdx",true,"observe-prompt-latency",{"id":268,"data":270,"body":51,"filePath":273,"digest":274,"rendered":275,"legacyId":281},{"title":271,"date":272,"description":50},"How to Observe Prompt Latency and Errors with Envoy",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/observe-prompt-latency.md","21914831adf6a520",{"html":55,"metadata":276},{"headings":277,"localImagePaths":278,"remoteImagePaths":279,"frontmatter":270,"imagePaths":280},[],[],[],[],"observe-prompt-latency.md","prompt-firewall",{"id":282,"data":284,"body":51,"filePath":287,"digest":288,"rendered":289,"legacyId":295},{"title":285,"date":286,"description":50},"Build a Prompt Firewall with Envoy Gateway",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/prompt-firewall.md","e174dade26d84e76",{"html":55,"metadata":290},{"headings":291,"localImagePaths":292,"remoteImagePaths":293,"frontmatter":284,"imagePaths":294},[],[],[],[],"prompt-firewall.md","rate-limit-openai-prompts",{"id":296,"data":298,"body":51,"filePath":301,"digest":302,"rendered":303,"legacyId":309},{"title":299,"date":300,"description":50},"Rate-Limit OpenAI Prompt Usage per Team",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/rate-limit-openai-prompts.md","1f19229934e87d59",{"html":55,"metadata":304},{"headings":305,"localImagePaths":306,"remoteImagePaths":307,"frontmatter":298,"imagePaths":308},[],[],[],[],"rate-limit-openai-prompts.md","mtls-internal-llms",{"id":310,"data":312,"body":51,"filePath":315,"digest":316,"rendered":317,"legacyId":323},{"title":313,"date":314,"description":50},"mTLS for Internal LLM APIs with Envoy Gateway",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/mtls-internal-llms.md","de5c1483dd91eb9a",{"html":55,"metadata":318},{"headings":319,"localImagePaths":320,"remoteImagePaths":321,"frontmatter":312,"imagePaths":322},[],[],[],[],"mtls-internal-llms.md","rise-of-llmops",{"id":324,"data":326,"body":51,"filePath":329,"digest":330,"rendered":331,"legacyId":337},{"title":327,"date":328,"description":50},"The Rise of LLMOps and Why It Needs a Gateway",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/rise-of-llmops.md","0ad01db78f4f619a",{"html":55,"metadata":332},{"headings":333,"localImagePaths":334,"remoteImagePaths":335,"frontmatter":326,"imagePaths":336},[],[],[],[],"rise-of-llmops.md","secure-ai-regulated",{"id":338,"data":340,"body":51,"filePath":343,"digest":344,"rendered":345,"legacyId":351},{"title":341,"date":342,"description":50},"Best Practices for Securing AI APIs in Regulated Industries",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/secure-ai-regulated.md","28c2cfe343e723eb",{"html":55,"metadata":346},{"headings":347,"localImagePaths":348,"remoteImagePaths":349,"frontmatter":340,"imagePaths":350},[],[],[],[],"secure-ai-regulated.md","secure-openai-oidc",{"id":352,"data":354,"body":51,"filePath":357,"digest":358,"rendered":359,"legacyId":365},{"title":355,"date":356,"description":50},"Secure OpenAI Access with OIDC in Kubernetes",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/secure-openai-oidc.md","bb1568cb0d4a744b",{"html":55,"metadata":360},{"headings":361,"localImagePaths":362,"remoteImagePaths":363,"frontmatter":354,"imagePaths":364},[],[],[],[],"secure-openai-oidc.md","self-service-ai",{"id":366,"data":368,"body":51,"filePath":371,"digest":372,"rendered":373,"legacyId":379},{"title":369,"date":370,"description":50},"Enabling Self-Service AI Access for Internal Devs",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/self-service-ai.md","8d3c3a7617b625ae",{"html":55,"metadata":374},{"headings":375,"localImagePaths":376,"remoteImagePaths":377,"frontmatter":368,"imagePaths":378},[],[],[],[],"self-service-ai.md","token-accounting-custom-llms",{"id":380,"data":382,"body":51,"filePath":385,"digest":386,"rendered":387,"legacyId":393},{"title":383,"date":384,"description":50},"Token Accounting for Custom LLM APIs (e.g. Mistral)",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/token-accounting-custom-llms.md","18d80e52c482aecd",{"html":55,"metadata":388},{"headings":389,"localImagePaths":390,"remoteImagePaths":391,"frontmatter":382,"imagePaths":392},[],[],[],[],"token-accounting-custom-llms.md","secure-hugging-face-tgi-with-envoy-ai-gateway",{"id":394,"data":396,"body":400,"filePath":401,"digest":402,"legacyId":403,"deferredRender":267},{"title":397,"date":398,"description":399},"Secure Hugging Face TGI with Envoy AI Gateway: Auth, Rate Limits, and Observability",["Date","2025-05-28T00:00:00.000Z"],"Deploy Hugging Face’s TGI behind Envoy AI Gateway to control access, enforce rate limits, and monitor usage in Kubernetes.","# Secure Hugging Face TGI with Envoy AI Gateway: Auth, Rate Limits, and Observability\n\nDeploying a hosted LLM like Hugging Face TGI is only the beginning. To make it usable in production, you need:\n\n- Authentication to restrict who can prompt the model\n- Rate limits to avoid accidental (or intentional) overload\n- Observability to know how it's being used\n\n**Envoy AI Gateway** makes all of that easy—and Kubernetes-native.\n\n## In This Tutorial, You'll Learn\n\n- How to deploy Hugging Face TGI in Kubernetes\n- How to route traffic through Envoy AI Gateway\n- How to apply mTLS or OIDC authentication\n- How to add per-user rate limiting\n- How to collect usage metrics for prompts and responses\n\n### Architecture Diagram\n```\n[ App ] --> [ Envoy AI Gateway ] --> [ Hugging Face TGI ]\n                 |                      |\n           Policies + Auth          Model Output\n```\n\n## Sample Gateway API Policy\n```yaml\nrateLimits:\n  - name: tgi-user-limit\n    match:\n      path: /generate\n    requestsPerUnit: 50\n    unit: minute\n```\n\n## Bonus Features\n\n- ✅ Use Prometheus and Grafana to chart token usage\n- ✅ Extend with Wasm filters to redact prompts or mask PII\n\n**[Deploy the Example](#)** | **[Join the Community](#)**","src/content/blog/secure-hugging-face-tgi-with-envoy-ai-gateway.mdx","1c9eb9e098ad6aed","secure-hugging-face-tgi-with-envoy-ai-gateway.mdx","secure-scalable-ai-inference-for-kubernetes",{"id":404,"data":406,"body":408,"filePath":409,"digest":410,"legacyId":411,"deferredRender":267},{"title":191,"date":407,"description":193},["Date","2025-05-28T00:00:00.000Z"],"# Introducing Envoy AI Gateway: Secure, Scalable AI Inference for Kubernetes\n\nAI models are powering everything from chatbots to copilots—but most teams are exposing them through insecure, unobservable, and expensive APIs. Envoy AI Gateway solves this by putting AI access behind a secure, policy-aware, and Kubernetes-native gateway.\n\n## The Problem\nProductionizing AI brings serious challenges:\n\n- No easy way to apply authentication or rate limits\n- Token sprawl and runaway inference costs\n- Lack of observability into model usage—or abuse\n\n## The Solution\n**Envoy AI Gateway** is built on Envoy Proxy and the Kubernetes Gateway API. It gives platform teams a modern, secure, and extensible way to manage AI inference traffic.\n\n### Key Benefits\n- ✅ Enforce mTLS, OIDC, and RBAC for LLM endpoints\n- ✅ Control usage with token-aware rate limits and quotas\n- ✅ Observe prompt traffic, latency, and errors\n- ✅ Native Kubernetes and GitOps support\n\nWhether you're managing OpenAI usage or serving your own models, Envoy AI Gateway helps you deliver AI securely, cost-effectively, and at scale.\n\n**[Get started](#)** | **[View architecture guide](#)** | **[Try the demo](#)**","src/content/blog/secure-scalable-ai-inference-for-kubernetes.mdx","c20e698069f1e327","secure-scalable-ai-inference-for-kubernetes.mdx","token-alerting",{"id":412,"data":414,"body":51,"filePath":417,"digest":418,"rendered":419,"legacyId":425},{"title":415,"date":416,"description":50},"How to Set Up Alerting on Token Overages",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/token-alerting.md","d8cc45b2c11226ab",{"html":55,"metadata":420},{"headings":421,"localImagePaths":422,"remoteImagePaths":423,"frontmatter":414,"imagePaths":424},[],[],[],[],"token-alerting.md","token-cost-prometheus",{"id":426,"data":428,"body":51,"filePath":431,"digest":432,"rendered":433,"legacyId":439},{"title":429,"date":430,"description":50},"Track and Visualize Token Costs with Prometheus + Grafana",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/token-cost-prometheus.md","7c15dd63a183db73",{"html":55,"metadata":434},{"headings":435,"localImagePaths":436,"remoteImagePaths":437,"frontmatter":428,"imagePaths":438},[],[],[],[],"token-cost-prometheus.md","top-envoy-filters",{"id":440,"data":442,"body":51,"filePath":445,"digest":446,"rendered":447,"legacyId":453},{"title":443,"date":444,"description":50},"7 Envoy Filters Every AI Gateway Should Use",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/top-envoy-filters.md","8ee7d4e050e9e36b",{"html":55,"metadata":448},{"headings":449,"localImagePaths":450,"remoteImagePaths":451,"frontmatter":442,"imagePaths":452},[],[],[],[],"top-envoy-filters.md","usage-tier-enforcement",{"id":454,"data":456,"body":51,"filePath":459,"digest":460,"rendered":461,"legacyId":467},{"title":457,"date":458,"description":50},"How to Enforce Usage Tiers for AI Services",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/usage-tier-enforcement.md","c3a62156444b55ca",{"html":55,"metadata":462},{"headings":463,"localImagePaths":464,"remoteImagePaths":465,"frontmatter":456,"imagePaths":466},[],[],[],[],"usage-tier-enforcement.md","wasm-prompt-sanitization",{"id":468,"data":470,"body":51,"filePath":473,"digest":474,"rendered":475,"legacyId":481},{"title":471,"date":472,"description":50},"Using Wasm in Envoy AI Gateway to Sanitize Prompts",["Date","2025-05-28T00:00:00.000Z"],"src/content/blog/wasm-prompt-sanitization.md","6136eebeb8302392",{"html":55,"metadata":476},{"headings":477,"localImagePaths":478,"remoteImagePaths":479,"frontmatter":470,"imagePaths":480},[],[],[],[],"wasm-prompt-sanitization.md","tutorial-02-track-and-limit-llm-token-usage-in-langchain-with-envoy-ai-gateway",{"id":482,"data":484,"body":488,"filePath":489,"digest":490,"legacyId":491,"deferredRender":267},{"title":485,"date":486,"description":487},"Track and Limit LLM Token Usage in LangChain with Envoy AI Gateway",["Date","2025-05-28T00:00:00.000Z"],"Use Envoy AI Gateway to enforce token-aware rate limits for LangChain applications.","# Track and Limit LLM Token Usage in LangChain with Envoy AI Gateway\n\nLangChain enables chaining prompts and agents—but that can burn through tokens fast. Use Envoy AI Gateway to:\n\n- Count tokens per request\n- Enforce daily or session-based token quotas\n- Log and block overuse in real-time\n\n## What You'll Learn\n\n- How to inspect prompt size and response tokens\n- How to attach a token budget to a user or agent ID\n- How to build dashboards using Prometheus and Grafana\n\n## Sample Token Budget Policy (Conceptual)\n```yaml\ntokenBudgets:\n  - user: user123\n    maxTokens: 10000\n    period: day\n```\n\n**Ideal for copilots, assistants, and research workloads where token budgets are critical.**","src/content/blog/tutorial-02-track-and-limit-llm-token-usage-in-langchain-with-envoy-ai-gateway.mdx","108258727391ff6f","tutorial-02-track-and-limit-llm-token-usage-in-langchain-with-envoy-ai-gateway.mdx",{"id":30,"data":493,"body":497,"filePath":498,"digest":499,"rendered":500,"legacyId":524},{"title":494,"date":495,"description":496},"What Is an AI Gateway and Why You Need One",["Date","2025-05-28T00:00:00.000Z"],"AI Gateways are designed to secure, control, and observe access to LLMs and model APIs. Here's why your platform team needs one.","# What Is an AI Gateway and Why You Need One\n\nAPI gateways have long been essential for securing and managing service traffic in microservice architectures. But AI workloads introduce fundamentally new challenges—ones that traditional gateways were never designed to handle.\n\nThat's where **AI Gateways** come in.\n\n## Why AI Workloads Need a New Kind of Gateway\n\nLarge language models (LLMs) and generative AI APIs differ from traditional services in three key ways:\n\n- **They’re expensive** — API calls often consume tokens billed by request, word, or model compute time.\n- **They’re sensitive** — LLMs may return user-generated data, require privacy controls, or demand robust authentication.\n- **They’re unpredictable** — With prompt engineering, model chaining, and multiple downstream services, usage patterns are hard to enforce without policy control.\n\nAs platform teams begin to productize AI for internal and external use, these issues become operational bottlenecks—and security risks.\n\n## What Does an AI Gateway Do?\n\nAn **AI Gateway** sits between clients (apps, developers, services) and your AI model endpoints (like OpenAI, Claude, or Hugging Face TGI). It provides:\n\n- 🔐 **Authentication and Authorization**  \n  Enforce OIDC, mTLS, and role-based access control (RBAC) per route or tenant.\n\n- ⏳ **Rate Limiting and Token Budgeting**  \n  Limit not just requests per minute, but also total tokens, prompt sizes, or cost-based quotas.\n\n- 📊 **Prompt Observability**  \n  Track who is sending prompts, how many tokens are used, what latency or error rates occur, and what content is passing through.\n\n- 🔍 **Policy Enforcement and Governance**  \n  Sanitize prompts, redact PII, block abuse patterns, and ensure LLM use aligns with company policies.\n\n## How Is It Different from a Traditional API Gateway?\n\nHere’s a quick comparison:\n\n| Feature                         | API Gateway | AI Gateway |\n|--------------------------------|-------------|------------|\n| Token-based rate limiting      | ❌          | ✅         |\n| Prompt observability           | ❌          | ✅         |\n| LLM-specific usage policies    | ❌          | ✅         |\n| OpenAI/HuggingFace compatibility | ❌        | ✅         |\n| Native Kubernetes integration  | ⚠️ (varies) | ✅         |\n| Support for multi-tenant access| ⚠️          | ✅         |\n\n## Use Cases for an AI Gateway\n\n- **Control OpenAI access** for internal developers with SSO and usage quotas.\n- **Expose Hugging Face models** to front-end apps with fine-grained policies.\n- **Monitor LLM cost usage** and set alerts before budgets are exceeded.\n- **Prevent prompt injection** and ensure safe, compliant prompt handling.\n\n## Meet Envoy AI Gateway\n\n**Envoy AI Gateway** is an open-source, Kubernetes-native solution built on Envoy Proxy and the Gateway API. It was designed specifically for the challenges of production AI.\n\nYou can use it to:\n- Secure model APIs with zero-trust security\n- Control costs with token-aware rate limiting\n- Gain visibility into AI usage across teams and tenants\n\nWhether you're building a platform for AI copilots, chatbots, document understanding, or agent workflows—**an AI Gateway should be your first line of defense and control**.\n\n---\n\n**📘 Ready to Try It?**  \n👉 [Get started with Envoy AI Gateway](#)  \n👉 [See the full tutorial series](#)","src/content/blog/what-is-an-ai-gateway.md","633eebafd2c2b93b",{"html":501,"metadata":502},"\u003Ch1 id=\"what-is-an-ai-gateway-and-why-you-need-one\">What Is an AI Gateway and Why You Need One\u003C/h1>\n\u003Cp>API gateways have long been essential for securing and managing service traffic in microservice architectures. But AI workloads introduce fundamentally new challenges—ones that traditional gateways were never designed to handle.\u003C/p>\n\u003Cp>That’s where \u003Cstrong>AI Gateways\u003C/strong> come in.\u003C/p>\n\u003Ch2 id=\"why-ai-workloads-need-a-new-kind-of-gateway\">Why AI Workloads Need a New Kind of Gateway\u003C/h2>\n\u003Cp>Large language models (LLMs) and generative AI APIs differ from traditional services in three key ways:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>They’re expensive\u003C/strong> — API calls often consume tokens billed by request, word, or model compute time.\u003C/li>\n\u003Cli>\u003Cstrong>They’re sensitive\u003C/strong> — LLMs may return user-generated data, require privacy controls, or demand robust authentication.\u003C/li>\n\u003Cli>\u003Cstrong>They’re unpredictable\u003C/strong> — With prompt engineering, model chaining, and multiple downstream services, usage patterns are hard to enforce without policy control.\u003C/li>\n\u003C/ul>\n\u003Cp>As platform teams begin to productize AI for internal and external use, these issues become operational bottlenecks—and security risks.\u003C/p>\n\u003Ch2 id=\"what-does-an-ai-gateway-do\">What Does an AI Gateway Do?\u003C/h2>\n\u003Cp>An \u003Cstrong>AI Gateway\u003C/strong> sits between clients (apps, developers, services) and your AI model endpoints (like OpenAI, Claude, or Hugging Face TGI). It provides:\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Cp>🔐 \u003Cstrong>Authentication and Authorization\u003C/strong>\u003Cbr>\nEnforce OIDC, mTLS, and role-based access control (RBAC) per route or tenant.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>⏳ \u003Cstrong>Rate Limiting and Token Budgeting\u003C/strong>\u003Cbr>\nLimit not just requests per minute, but also total tokens, prompt sizes, or cost-based quotas.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>📊 \u003Cstrong>Prompt Observability\u003C/strong>\u003Cbr>\nTrack who is sending prompts, how many tokens are used, what latency or error rates occur, and what content is passing through.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>🔍 \u003Cstrong>Policy Enforcement and Governance\u003C/strong>\u003Cbr>\nSanitize prompts, redact PII, block abuse patterns, and ensure LLM use aligns with company policies.\u003C/p>\n\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"how-is-it-different-from-a-traditional-api-gateway\">How Is It Different from a Traditional API Gateway?\u003C/h2>\n\u003Cp>Here’s a quick comparison:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Feature\u003C/th>\u003Cth>API Gateway\u003C/th>\u003Cth>AI Gateway\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Token-based rate limiting\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Prompt observability\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>LLM-specific usage policies\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>OpenAI/HuggingFace compatibility\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Native Kubernetes integration\u003C/td>\u003Ctd>⚠️ (varies)\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Support for multi-tenant access\u003C/td>\u003Ctd>⚠️\u003C/td>\u003Ctd>✅\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Ch2 id=\"use-cases-for-an-ai-gateway\">Use Cases for an AI Gateway\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Control OpenAI access\u003C/strong> for internal developers with SSO and usage quotas.\u003C/li>\n\u003Cli>\u003Cstrong>Expose Hugging Face models\u003C/strong> to front-end apps with fine-grained policies.\u003C/li>\n\u003Cli>\u003Cstrong>Monitor LLM cost usage\u003C/strong> and set alerts before budgets are exceeded.\u003C/li>\n\u003Cli>\u003Cstrong>Prevent prompt injection\u003C/strong> and ensure safe, compliant prompt handling.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"meet-envoy-ai-gateway\">Meet Envoy AI Gateway\u003C/h2>\n\u003Cp>\u003Cstrong>Envoy AI Gateway\u003C/strong> is an open-source, Kubernetes-native solution built on Envoy Proxy and the Gateway API. It was designed specifically for the challenges of production AI.\u003C/p>\n\u003Cp>You can use it to:\u003C/p>\n\u003Cul>\n\u003Cli>Secure model APIs with zero-trust security\u003C/li>\n\u003Cli>Control costs with token-aware rate limiting\u003C/li>\n\u003Cli>Gain visibility into AI usage across teams and tenants\u003C/li>\n\u003C/ul>\n\u003Cp>Whether you’re building a platform for AI copilots, chatbots, document understanding, or agent workflows—\u003Cstrong>an AI Gateway should be your first line of defense and control\u003C/strong>.\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cstrong>📘 Ready to Try It?\u003C/strong>\u003Cbr>\n👉 \u003Ca href=\"#\">Get started with Envoy AI Gateway\u003C/a>\u003Cbr>\n👉 \u003Ca href=\"#\">See the full tutorial series\u003C/a>\u003C/p>",{"headings":503,"localImagePaths":521,"remoteImagePaths":522,"frontmatter":493,"imagePaths":523},[504,506,509,512,515,518],{"depth":23,"slug":505,"text":494},"what-is-an-ai-gateway-and-why-you-need-one",{"depth":26,"slug":507,"text":508},"why-ai-workloads-need-a-new-kind-of-gateway","Why AI Workloads Need a New Kind of Gateway",{"depth":26,"slug":510,"text":511},"what-does-an-ai-gateway-do","What Does an AI Gateway Do?",{"depth":26,"slug":513,"text":514},"how-is-it-different-from-a-traditional-api-gateway","How Is It Different from a Traditional API Gateway?",{"depth":26,"slug":516,"text":517},"use-cases-for-an-ai-gateway","Use Cases for an AI Gateway",{"depth":26,"slug":519,"text":520},"meet-envoy-ai-gateway","Meet Envoy AI Gateway",[],[],[],"what-is-an-ai-gateway.md","what-is-an-ai-gateway-and-why-do-you-need-one",{"id":525,"data":527,"body":531,"filePath":532,"digest":533,"legacyId":534,"deferredRender":267},{"title":528,"date":529,"description":530},"What Is an AI Gateway and Why Do You Need One?",["Date","2025-05-28T00:00:00.000Z"],"Understand the purpose of AI Gateways and why modern AI infrastructure needs policy-aware model traffic control.","# What Is an AI Gateway and Why Do You Need One?\n\nYou’ve secured your microservices with ingress controllers. You’ve adopted API gateways for external traffic. But what about your AI models?\n\nTraditional gateways weren't built for:\n\n- Prompt payloads that consume tokens and dollars\n- Models that need usage controls across tenants\n- Observability of LLM inputs and outputs\n\n## Enter the AI Gateway\nAn **AI Gateway** sits in front of model endpoints—like OpenAI, Claude, or internal Hugging Face models—and provides:\n\n- 🔐 Authentication and authorization (OIDC, mTLS)\n- ⏱️ Rate limiting and token budgets\n- 📊 Prompt observability and usage tracking\n- 📜 Policy enforcement and traffic governance\n\n## Do You Need One?\nYou do, if:\n\n- You have multiple teams accessing shared LLMs\n- You’re worried about API abuse, quota blowouts, or compliance\n- You want to scale your AI platform with confidence\n\n**Envoy AI Gateway** was built for this exact use case.\n\n**[Explore use cases](#)** | **[Read the Getting Started guide](#)** | **[Compare AI Gateways](#)**","src/content/blog/what-is-an-ai-gateway-and-why-do-you-need-one.mdx","eb8e3798b135ff9f","what-is-an-ai-gateway-and-why-do-you-need-one.mdx","tutorial-01-enforce-oidc-auth-for-openai-with-envoy-ai-gateway",{"id":535,"data":537,"body":541,"filePath":542,"digest":543,"legacyId":544,"deferredRender":267},{"title":538,"date":539,"description":540},"Authenticate OpenAI Traffic in Kubernetes with OIDC and Envoy AI Gateway",["Date","2025-05-28T00:00:00.000Z"],"Protect OpenAI access with OIDC-based authentication via Envoy AI Gateway in Kubernetes.","# Authenticate OpenAI Traffic in Kubernetes with OIDC and Envoy AI Gateway\n\nUncontrolled access to OpenAI can lead to shadow usage, security gaps, and surprise bills. By enforcing OIDC via Envoy AI Gateway, you can ensure that only authenticated and authorized users are able to access LLM endpoints.\n\n## What You'll Learn\n\n- How to configure Envoy AI Gateway with OIDC (e.g., Auth0, Okta, Google)\n- How to intercept OpenAI requests and enforce login\n- How to forward JWT claims for audit and policy control\n\n## Use Case\nYou want your internal dev portal to require SSO login before using GPT-based tools. You also want to:\n\n- Apply tenant-based rate limits\n- Track usage per authenticated user\n\n## Sample Policy Snippet\n```yaml\nauthentication:\n  - provider:\n      name: oidc-auth\n      issuer: https://auth.myorg.com\n      clientId: envoy-gateway\n      jwksUri: https://auth.myorg.com/.well-known/jwks.json\n```\n\n**Secure model access, maintain audit trails, and support zero trust architecture.**","src/content/blog/tutorial-01-enforce-oidc-auth-for-openai-with-envoy-ai-gateway.mdx","f0895a4576c339c1","tutorial-01-enforce-oidc-auth-for-openai-with-envoy-ai-gateway.mdx","tutorial-03-protect-internal-model-apis-seldon-bentoml-with-mtls-in-envoy-gateway",{"id":545,"data":547,"body":551,"filePath":552,"digest":553,"legacyId":554,"deferredRender":267},{"title":548,"date":549,"description":550},"Protect Internal Model APIs (Seldon, BentoML) with mTLS in Envoy AI Gateway",["Date","2025-05-28T00:00:00.000Z"],"Use mTLS in Envoy AI Gateway to secure internal ML model APIs and enforce zero-trust access.","# Protect Internal Model APIs (Seldon, BentoML) with mTLS in Envoy AI Gateway\n\nServing internal models in Kubernetes—via Seldon, BentoML, or Ray Serve—requires more than just network policies. Use Envoy AI Gateway to:\n\n- Enforce client identity with mTLS\n- Encrypt model traffic end-to-end\n- Prevent unauthorized service access\n\n## What You'll Learn\n\n- How to generate and sign client certificates\n- How to enable mTLS on Gateway routes\n- How to rotate certificates securely\n\n## Sample TLS Policy Snippet\n```yaml\ntls:\n  mode: MUTUAL\n  clientCertificateAuthorities:\n    - secretRef: seldon-ca-secret\n```\n\n**Raise the bar on internal security—without custom firewall rules or external proxies.**","src/content/blog/tutorial-03-protect-internal-model-apis-seldon-bentoml-with-mtls-in-envoy-gateway.mdx","738493786059479c","tutorial-03-protect-internal-model-apis-seldon-bentoml-with-mtls-in-envoy-gateway.mdx"]